{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from configparser import ConfigParser, ExtendedInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    start = time.time()\n",
    "    print ('Running setup...')\n",
    "    tqdm.pandas()\n",
    "    resources = ['taggers/averaged_perceptron_tagger', 'corpora/wordnet', 'corpora/stopwords', 'tokenizers/punkt']\n",
    "    for path in resources:\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            nltk.download(path.split('/')[1])\n",
    "    end = time.time()\n",
    "    print (f'Setup finished in {end-start:.2f} seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup...\n",
      "Setup finished in 0.03 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config.ini']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser(\n",
    "    inline_comment_prefixes=\"#;\",\n",
    "    interpolation=ExtendedInterpolation())\n",
    "config.read('../config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = config['General']['input_file']\n",
    "output = config['Text Cleaning']['tokenized_file']\n",
    "text_column = config['General']['input_file_text_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/twitter.csv', './data/tokenized.data', 'Tweet')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input, output, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = '../data/twitter.csv'\n",
    "output = '../data/tokenized.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_date(srl_no):\n",
    "    args = list(map(int, [number.strip() for number in config['General']['first_date'].split(',')]))\n",
    "    first = datetime.datetime(args[0], args[1], args[2])\n",
    "    days = int(srl_no-1)\n",
    "    new_date = first + datetime.timedelta(days)\n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    print('Reading data...')\n",
    "    start = time.time()\n",
    "    tweets = pd.read_csv(filename, encoding='latin-1', sep=';',header=0, names=['StringDate', 'Days', 'From', 'Tweet'])\n",
    "    tweets = tweets.filter(items=['Days', 'From', 'Tweet'])\n",
    "    if config.getboolean('General', 'convert_date'):\n",
    "        tweets['Days'] = tweets['Days'].progress_apply(days_to_date)\n",
    "    tweets.columns=['Date', 'From', 'Tweet']\n",
    "    tweets['From']= tweets['From'].apply(lambda x: x.lower())\n",
    "    tweets = tweets[~tweets['From']\n",
    "        .isin(\n",
    "            [source.strip().lower()\n",
    "                for source in \n",
    "                    config['General']['exclude_sources'].split(\",\")])]\n",
    "    tweets = tweets.set_index('Date', drop=True).sort_index()\n",
    "    end = time.time()\n",
    "    print (f'Data read in {end-start:.2f} seconds.\\n')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:11<00:00, 53945.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read in 14.83 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data = read_data(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    return str(text).lower()\n",
    "    \n",
    "def remove_accents(text):\n",
    "    text = unicodedata.normalize('NFD', str(text)).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def remove_apostrophes(text):\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    #hashtags and handles\n",
    "    text = re.sub(r'\\B(\\#([0-9]|[a-zA-Z])+|\\@([0-9]|[a-zA-Z])+\\b)', '', text)\n",
    "    return text\n",
    "def remove_urls(text):\n",
    "    text= re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_numberwords(text):\n",
    "    text= re.sub(r'\\b[0-9]+\\b\\s*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(df, text_column):\n",
    "    tqdm.write(f'Cleaning up {text_column} texts...')\n",
    "    start = time.time()\n",
    "    actions = [action.strip().lower() for action in config['Text Cleaning']['actions'].split(\",\")]\n",
    "    for action in actions:\n",
    "        tqdm.write('.... ' + action)\n",
    "        df[text_column] = df[text_column].progress_apply(globals()[action])\n",
    "    end = time.time()\n",
    "    tqdm.write(f'Text cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18593/518145 [00:00<00:02, 185928.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Tweet texts...\n",
      ".... lower_case\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:01<00:00, 354816.76it/s]\n",
      " 11%|█▏        | 58592/518145 [00:00<00:01, 268926.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_accents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:01<00:00, 319550.24it/s]\n",
      " 10%|█         | 52214/518145 [00:00<00:01, 246780.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_apostrophes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:01<00:00, 294789.32it/s]\n",
      "  5%|▌         | 26311/518145 [00:00<00:03, 125367.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_hashtags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:04<00:00, 125530.48it/s]\n",
      "  1%|▏         | 7087/518145 [00:00<00:07, 69536.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:02<00:00, 193436.70it/s]\n",
      "  1%|▏         | 7770/518145 [00:00<00:06, 77418.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_numberwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:03<00:00, 158507.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleanup finished in 15.41 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned = clean_text(input_data, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, text_column):\n",
    "    print(f'Tokenizing Dataframe[\"{text_column}\"].')\n",
    "    start = time.time()\n",
    "    df['Unigrams'] = df[text_column].progress_apply(tknzr.tokenize)\n",
    "    end = time.time()\n",
    "    print(f'Dataframe[\"{text_column}\"] tokenized in {end-start:.2f} seconds.\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stopwords(input, stops):\n",
    "    output = [i for i in input if i not in stops]\n",
    "    return output\n",
    "\n",
    "def remove_extremewords(input, min, max):\n",
    "    output = [i for i in input if (len(i)<=max and len(i)>=min)]\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_tokens(df):\n",
    "    tqdm.write('Cleaning up tokens...')\n",
    "    start = time.time()\n",
    "    tqdm.write('.... removing extreme words')\n",
    "    min = config['Text Cleaning'].getint('min_word_size') or 2\n",
    "    max = config['Text Cleaning'].getint('max_word_size') or 20\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(lambda x: remove_extremewords(x, min, max))\n",
    "    tqdm.write('.... removing stop words')\n",
    "    ll = [stopwords.words('english') + list(punctuation)] + [\"\".join(string.split()).split(',') for string in [v for k, v in config.items('Stop Words')]]\n",
    "    flat = [item for sublist in ll for item in sublist]\n",
    "    stops = set(flat)\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(lambda x: remove_stopwords(input=x, stops=stops))\n",
    "    tqdm.write('.... generating bigrams')\n",
    "    df['Bigrams'] = df['Unigrams'].progress_apply(lambda x: [f'{tuple[0]} {tuple[1]}' for tuple in list(bigrams(x))])\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Tokens cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 943/518145 [00:00<00:54, 9421.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Dataframe[\"Tweet\"].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:46<00:00, 11238.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe[\"Tweet\"] tokenized in 46.18 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenize(cleaned, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Unigrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@sai</td>\n",
       "      <td>top objectively biggest tech stories of</td>\n",
       "      <td>[top, objectively, biggest, tech, stories, of]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>silicon valley campaign seeks startup visa for...</td>\n",
       "      <td>[silicon, valley, campaign, seeks, startup, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>: my fifth annual list of the tech products i ...</td>\n",
       "      <td>[:, my, fifth, annual, list, of, the, tech, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>namebench: google % project to find the fastes...</td>\n",
       "      <td>[namebench, :, google, %, project, to, find, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>six new years resolutions for apple and the ip...</td>\n",
       "      <td>[six, new, years, resolutions, for, apple, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>ten technologies that will rock by</td>\n",
       "      <td>[ten, technologies, that, will, rock, by]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>hotel wifi should be a right, not a luxury  by</td>\n",
       "      <td>[hotel, wifi, should, be, a, right, ,, not, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>twitter and me!  why it the only social media ...</td>\n",
       "      <td>[twitter, and, me, !, why, it, the, only, soci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>ben heck ps3 slim laptop pops up for sale on e...</td>\n",
       "      <td>[ben, heck, ps3, slim, laptop, pops, up, for, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>predictions for ?</td>\n",
       "      <td>[predictions, for, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>a new year gift to engadget readers: minutes o...</td>\n",
       "      <td>[a, new, year, gift, to, engadget, readers, :,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>us government launches distraction.gov, wants ...</td>\n",
       "      <td>[us, government, launches, distraction.gov, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@sai</td>\n",
       "      <td>apple expects to sell million tablets in first...</td>\n",
       "      <td>[apple, expects, to, sell, million, tablets, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>ocz vertex pro ssd previewed: awesome, and cou...</td>\n",
       "      <td>[ocz, vertex, pro, ssd, previewed, :, awesome,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>netgear ptv1000 push tv wifi video card hits t...</td>\n",
       "      <td>[netgear, ptv, 1000, push, tv, wifi, video, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@bbctech</td>\n",
       "      <td>will the evolution of smartphones see off sat-...</td>\n",
       "      <td>[will, the, evolution, of, smartphones, see, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@techreview</td>\n",
       "      <td>physics arxiv blog highlights of : september: ...</td>\n",
       "      <td>[physics, arxiv, blog, highlights, of, :, sept...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>compatibility test: facebook | guy browning</td>\n",
       "      <td>[compatibility, test, :, facebook, |, guy, bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>children of the virtual world</td>\n",
       "      <td>[children, of, the, virtual, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>a new decade: what in a name?</td>\n",
       "      <td>[a, new, decade, :, what, in, a, name, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>racial or religious groups could be picked out...</td>\n",
       "      <td>[racial, or, religious, groups, could, be, pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>the most viewed stories and galleries on techn...</td>\n",
       "      <td>[the, most, viewed, stories, and, galleries, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@reuterstech</td>\n",
       "      <td>taiwan asustek to free up subsidiary by june -...</td>\n",
       "      <td>[taiwan, asustek, to, free, up, subsidiary, by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@huffposttech</td>\n",
       "      <td>ultimate guide to apple tablet (or itablet or ...</td>\n",
       "      <td>[ultimate, guide, to, apple, tablet, (, or, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>the most intriguing games of : part two</td>\n",
       "      <td>[the, most, intriguing, games, of, :, part, two]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>ocado ipo prospects grow: the prospects of onl...</td>\n",
       "      <td>[ocado, ipo, prospects, grow, :, the, prospect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@sai</td>\n",
       "      <td>fox -time warner spat shows why the death of t...</td>\n",
       "      <td>[fox, -, time, warner, spat, shows, why, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>china mobile communications executive sacked f...</td>\n",
       "      <td>[china, mobile, communications, executive, sac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>at&amp;t severs ties with woods: at&amp;t has dropped ...</td>\n",
       "      <td>[at, &amp;, t, severs, ties, with, woods, :, at, &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>@sai</td>\n",
       "      <td>googler running for governor of vermont</td>\n",
       "      <td>[googler, running, for, governor, of, vermont]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>heres how mouse support could change the way y...</td>\n",
       "      <td>[heres, how, mouse, support, could, change, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>after math: jony bye-ve</td>\n",
       "      <td>[after, math, :, jony, bye-ve]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>the best sous vide machine and gear</td>\n",
       "      <td>[the, best, sous, vide, machine, and, gear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>amd denies improperly sharing cpu tech with ch...</td>\n",
       "      <td>[amd, denies, improperly, sharing, cpu, tech, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@wired</td>\n",
       "      <td>behold a sports car combination made in heaven...</td>\n",
       "      <td>[behold, a, sports, car, combination, made, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@wired</td>\n",
       "      <td>psa: you don't actually own things like your i...</td>\n",
       "      <td>[psa, :, you, don't, actually, own, things, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@wired</td>\n",
       "      <td>black holes are one of the galaxy greatest mys...</td>\n",
       "      <td>[black, holes, are, one, of, the, galaxy, grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>china eyes $16bn boost for tech sector with na...</td>\n",
       "      <td>[china, eyes, $, 16bn, boost, for, tech, secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>scientists $250m fund aims to keep start-ups i...</td>\n",
       "      <td>[scientists, $, 250m, fund, aims, to, keep, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@futureshift</td>\n",
       "      <td>revolut lets you round up payments and donate ...</td>\n",
       "      <td>[revolut, lets, you, round, up, payments, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>the robots are definitely coming and will make...</td>\n",
       "      <td>[the, robots, are, definitely, coming, and, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>amazon jeff bezos pays out $38bn in divorce se...</td>\n",
       "      <td>[amazon, jeff, bezos, pays, out, $, 38bn, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>white house insists trump huawei reversal not ...</td>\n",
       "      <td>[white, house, insists, trump, huawei, reversa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>apple music has surpassed million subscribers,...</td>\n",
       "      <td>[apple, music, has, surpassed, million, subscr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>pre-saving albums on spotify gives record labe...</td>\n",
       "      <td>[pre-saving, albums, on, spotify, gives, recor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>is a slider phone better than a notch?</td>\n",
       "      <td>[is, a, slider, phone, better, than, a, notch, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>southwest and united airlines extend max cance...</td>\n",
       "      <td>[southwest, and, united, airlines, extend, max...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>apple will repair macbook airs with faulty log...</td>\n",
       "      <td>[apple, will, repair, macbook, airs, with, fau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>google accused of inappropriate access to medi...</td>\n",
       "      <td>[google, accused, of, inappropriate, access, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@technology</td>\n",
       "      <td>elizabeth warrens proposal to break up big tec...</td>\n",
       "      <td>[elizabeth, warrens, proposal, to, break, up, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>displayport .is ready for 8k monitors and beyo...</td>\n",
       "      <td>[displayport, ., is, ready, for, 8k, monitors,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>theranos founder elizabeth holmes will go on t...</td>\n",
       "      <td>[theranos, founder, elizabeth, holmes, will, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>an amazon engineer made an ai-powered cat flap...</td>\n",
       "      <td>[an, amazon, engineer, made, an, ai-powered, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@darpa</td>\n",
       "      <td>mark rosker, the director of our microsystems ...</td>\n",
       "      <td>[mark, rosker, ,, the, director, of, our, micr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@verge</td>\n",
       "      <td>play with synths in your browser with abletons...</td>\n",
       "      <td>[play, with, synths, in, your, browser, with, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>spacex is still in control of all but three of...</td>\n",
       "      <td>[spacex, is, still, in, control, of, all, but,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>countries back plan to create 'free flow' of d...</td>\n",
       "      <td>[countries, back, plan, to, create, ', free, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@engadget</td>\n",
       "      <td>the new raspberry pi is ready for 4k video</td>\n",
       "      <td>[the, new, raspberry, pi, is, ready, for, 4k, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@wired</td>\n",
       "      <td>this is what san francisco looked like before ...</td>\n",
       "      <td>[this, is, what, san, francisco, looked, like,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>@techreview</td>\n",
       "      <td>fifty years after neil armstrong stepped onto ...</td>\n",
       "      <td>[fifty, years, after, neil, armstrong, stepped...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518145 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     From                                              Tweet  \\\n",
       "Date                                                                           \n",
       "2010-01-01           @sai           top objectively biggest tech stories of    \n",
       "2010-01-01  @guardiantech  silicon valley campaign seeks startup visa for...   \n",
       "2010-01-01    @techcrunch  : my fifth annual list of the tech products i ...   \n",
       "2010-01-01    @techcrunch  namebench: google % project to find the fastes...   \n",
       "2010-01-01    @techcrunch  six new years resolutions for apple and the ip...   \n",
       "2010-01-01    @techcrunch                ten technologies that will rock by    \n",
       "2010-01-01    @techcrunch    hotel wifi should be a right, not a luxury  by    \n",
       "2010-01-01    @techcrunch  twitter and me!  why it the only social media ...   \n",
       "2010-01-01      @engadget  ben heck ps3 slim laptop pops up for sale on e...   \n",
       "2010-01-01      @engadget                                 predictions for ?    \n",
       "2010-01-01      @engadget  a new year gift to engadget readers: minutes o...   \n",
       "2010-01-01      @engadget  us government launches distraction.gov, wants ...   \n",
       "2010-01-01           @sai  apple expects to sell million tablets in first...   \n",
       "2010-01-01      @engadget  ocz vertex pro ssd previewed: awesome, and cou...   \n",
       "2010-01-01      @engadget  netgear ptv1000 push tv wifi video card hits t...   \n",
       "2010-01-01       @bbctech  will the evolution of smartphones see off sat-...   \n",
       "2010-01-01    @techreview  physics arxiv blog highlights of : september: ...   \n",
       "2010-01-01  @guardiantech       compatibility test: facebook | guy browning    \n",
       "2010-01-01  @guardiantech                     children of the virtual world    \n",
       "2010-01-01  @guardiantech                     a new decade: what in a name?    \n",
       "2010-01-01  @guardiantech  racial or religious groups could be picked out...   \n",
       "2010-01-01  @guardiantech  the most viewed stories and galleries on techn...   \n",
       "2010-01-01   @reuterstech  taiwan asustek to free up subsidiary by june -...   \n",
       "2010-01-01  @huffposttech  ultimate guide to apple tablet (or itablet or ...   \n",
       "2010-01-01  @guardiantech           the most intriguing games of : part two    \n",
       "2010-01-01    @fttechnews  ocado ipo prospects grow: the prospects of onl...   \n",
       "2010-01-01           @sai  fox -time warner spat shows why the death of t...   \n",
       "2010-01-01    @fttechnews  china mobile communications executive sacked f...   \n",
       "2010-01-01    @fttechnews  at&t severs ties with woods: at&t has dropped ...   \n",
       "2010-01-01           @sai           googler running for governor of vermont    \n",
       "...                   ...                                                ...   \n",
       "2019-06-30         @verge  heres how mouse support could change the way y...   \n",
       "2019-06-30      @engadget                          after math: jony bye-ve     \n",
       "2019-06-30      @engadget              the best sous vide machine and gear     \n",
       "2019-06-30      @engadget  amd denies improperly sharing cpu tech with ch...   \n",
       "2019-06-30         @wired  behold a sports car combination made in heaven...   \n",
       "2019-06-30         @wired  psa: you don't actually own things like your i...   \n",
       "2019-06-30         @wired  black holes are one of the galaxy greatest mys...   \n",
       "2019-06-30    @fttechnews  china eyes $16bn boost for tech sector with na...   \n",
       "2019-06-30    @fttechnews  scientists $250m fund aims to keep start-ups i...   \n",
       "2019-06-30   @futureshift  revolut lets you round up payments and donate ...   \n",
       "2019-06-30  @guardiantech  the robots are definitely coming and will make...   \n",
       "2019-06-30  @guardiantech  amazon jeff bezos pays out $38bn in divorce se...   \n",
       "2019-06-30  @guardiantech  white house insists trump huawei reversal not ...   \n",
       "2019-06-30         @verge  apple music has surpassed million subscribers,...   \n",
       "2019-06-30         @verge  pre-saving albums on spotify gives record labe...   \n",
       "2019-06-30         @verge             is a slider phone better than a notch?   \n",
       "2019-06-30         @verge  southwest and united airlines extend max cance...   \n",
       "2019-06-30         @verge  apple will repair macbook airs with faulty log...   \n",
       "2019-06-30         @verge  google accused of inappropriate access to medi...   \n",
       "2019-06-30    @technology  elizabeth warrens proposal to break up big tec...   \n",
       "2019-06-30         @verge  displayport .is ready for 8k monitors and beyo...   \n",
       "2019-06-30         @verge  theranos founder elizabeth holmes will go on t...   \n",
       "2019-06-30         @verge  an amazon engineer made an ai-powered cat flap...   \n",
       "2019-06-30         @darpa  mark rosker, the director of our microsystems ...   \n",
       "2019-06-30         @verge  play with synths in your browser with abletons...   \n",
       "2019-06-30      @engadget  spacex is still in control of all but three of...   \n",
       "2019-06-30      @engadget  countries back plan to create 'free flow' of d...   \n",
       "2019-06-30      @engadget         the new raspberry pi is ready for 4k video   \n",
       "2019-06-30         @wired  this is what san francisco looked like before ...   \n",
       "2019-06-30    @techreview  fifty years after neil armstrong stepped onto ...   \n",
       "\n",
       "                                                     Unigrams  \n",
       "Date                                                           \n",
       "2010-01-01     [top, objectively, biggest, tech, stories, of]  \n",
       "2010-01-01  [silicon, valley, campaign, seeks, startup, vi...  \n",
       "2010-01-01  [:, my, fifth, annual, list, of, the, tech, pr...  \n",
       "2010-01-01  [namebench, :, google, %, project, to, find, t...  \n",
       "2010-01-01  [six, new, years, resolutions, for, apple, and...  \n",
       "2010-01-01          [ten, technologies, that, will, rock, by]  \n",
       "2010-01-01  [hotel, wifi, should, be, a, right, ,, not, a,...  \n",
       "2010-01-01  [twitter, and, me, !, why, it, the, only, soci...  \n",
       "2010-01-01  [ben, heck, ps3, slim, laptop, pops, up, for, ...  \n",
       "2010-01-01                              [predictions, for, ?]  \n",
       "2010-01-01  [a, new, year, gift, to, engadget, readers, :,...  \n",
       "2010-01-01  [us, government, launches, distraction.gov, ,,...  \n",
       "2010-01-01  [apple, expects, to, sell, million, tablets, i...  \n",
       "2010-01-01  [ocz, vertex, pro, ssd, previewed, :, awesome,...  \n",
       "2010-01-01  [netgear, ptv, 1000, push, tv, wifi, video, ca...  \n",
       "2010-01-01  [will, the, evolution, of, smartphones, see, o...  \n",
       "2010-01-01  [physics, arxiv, blog, highlights, of, :, sept...  \n",
       "2010-01-01  [compatibility, test, :, facebook, |, guy, bro...  \n",
       "2010-01-01                [children, of, the, virtual, world]  \n",
       "2010-01-01          [a, new, decade, :, what, in, a, name, ?]  \n",
       "2010-01-01  [racial, or, religious, groups, could, be, pic...  \n",
       "2010-01-01  [the, most, viewed, stories, and, galleries, o...  \n",
       "2010-01-01  [taiwan, asustek, to, free, up, subsidiary, by...  \n",
       "2010-01-01  [ultimate, guide, to, apple, tablet, (, or, it...  \n",
       "2010-01-01   [the, most, intriguing, games, of, :, part, two]  \n",
       "2010-01-01  [ocado, ipo, prospects, grow, :, the, prospect...  \n",
       "2010-01-01  [fox, -, time, warner, spat, shows, why, the, ...  \n",
       "2010-01-01  [china, mobile, communications, executive, sac...  \n",
       "2010-01-01  [at, &, t, severs, ties, with, woods, :, at, &...  \n",
       "2010-01-01     [googler, running, for, governor, of, vermont]  \n",
       "...                                                       ...  \n",
       "2019-06-30  [heres, how, mouse, support, could, change, th...  \n",
       "2019-06-30                     [after, math, :, jony, bye-ve]  \n",
       "2019-06-30        [the, best, sous, vide, machine, and, gear]  \n",
       "2019-06-30  [amd, denies, improperly, sharing, cpu, tech, ...  \n",
       "2019-06-30  [behold, a, sports, car, combination, made, in...  \n",
       "2019-06-30  [psa, :, you, don't, actually, own, things, li...  \n",
       "2019-06-30  [black, holes, are, one, of, the, galaxy, grea...  \n",
       "2019-06-30  [china, eyes, $, 16bn, boost, for, tech, secto...  \n",
       "2019-06-30  [scientists, $, 250m, fund, aims, to, keep, st...  \n",
       "2019-06-30  [revolut, lets, you, round, up, payments, and,...  \n",
       "2019-06-30  [the, robots, are, definitely, coming, and, wi...  \n",
       "2019-06-30  [amazon, jeff, bezos, pays, out, $, 38bn, in, ...  \n",
       "2019-06-30  [white, house, insists, trump, huawei, reversa...  \n",
       "2019-06-30  [apple, music, has, surpassed, million, subscr...  \n",
       "2019-06-30  [pre-saving, albums, on, spotify, gives, recor...  \n",
       "2019-06-30  [is, a, slider, phone, better, than, a, notch, ?]  \n",
       "2019-06-30  [southwest, and, united, airlines, extend, max...  \n",
       "2019-06-30  [apple, will, repair, macbook, airs, with, fau...  \n",
       "2019-06-30  [google, accused, of, inappropriate, access, t...  \n",
       "2019-06-30  [elizabeth, warrens, proposal, to, break, up, ...  \n",
       "2019-06-30  [displayport, ., is, ready, for, 8k, monitors,...  \n",
       "2019-06-30  [theranos, founder, elizabeth, holmes, will, g...  \n",
       "2019-06-30  [an, amazon, engineer, made, an, ai-powered, c...  \n",
       "2019-06-30  [mark, rosker, ,, the, director, of, our, micr...  \n",
       "2019-06-30  [play, with, synths, in, your, browser, with, ...  \n",
       "2019-06-30  [spacex, is, still, in, control, of, all, but,...  \n",
       "2019-06-30  [countries, back, plan, to, create, ', free, f...  \n",
       "2019-06-30  [the, new, raspberry, pi, is, ready, for, 4k, ...  \n",
       "2019-06-30  [this, is, what, san, francisco, looked, like,...  \n",
       "2019-06-30  [fifty, years, after, neil, armstrong, stepped...  \n",
       "\n",
       "[518145 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/518145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tokens...\n",
      ".... removing extreme words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:03<00:00, 130251.20it/s]\n",
      "  3%|▎         | 16832/518145 [00:00<00:02, 168316.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:02<00:00, 190498.01it/s]\n",
      "  4%|▍         | 23197/518145 [00:00<00:04, 109908.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... generating bigrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:04<00:00, 109532.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens cleanup finished in 11.80 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens(tokenized).to_pickle(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
