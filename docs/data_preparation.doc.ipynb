{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from configparser import ConfigParser, ExtendedInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    start = time.time()\n",
    "    print ('Running setup...')\n",
    "    tqdm.pandas()\n",
    "    resources = ['taggers/averaged_perceptron_tagger', 'corpora/wordnet', 'corpora/stopwords', 'tokenizers/punkt']\n",
    "    for path in resources:\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            nltk.download(path.split('/')[1])\n",
    "    end = time.time()\n",
    "    print (f'Setup finished in {end-start:.2f} seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup...\n",
      "Setup finished in 0.00 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config.ini']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser(\n",
    "    inline_comment_prefixes=\"#;\",\n",
    "    interpolation=ExtendedInterpolation())\n",
    "config.read('../config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = config['General']['input_file']\n",
    "output = config['Text Cleaning']['tokenized_file']\n",
    "text_column = config['General']['input_file_text_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/twitter.csv', './data/tokenized.data', 'Tweet')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input, output, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = '../data/twitter.csv'\n",
    "output = '../data/tokenized.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_date(srl_no):\n",
    "    args = list(map(int, [number.strip() for number in config['General']['first_date'].split(',')]))\n",
    "    first = datetime.datetime(args[0], args[1], args[2])\n",
    "    days = int(srl_no-1)\n",
    "    new_date = first + datetime.timedelta(days)\n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    print('Reading data...')\n",
    "    start = time.time()\n",
    "    tweets = pd.read_csv(filename, encoding='latin-1', sep=';',header=0, names=['StringDate', 'Days', 'From', 'Tweet'])\n",
    "    tweets = tweets.filter(items=['Days', 'From', 'Tweet'])\n",
    "    if config.getboolean('General', 'convert_date'):\n",
    "        tweets['Days'] = tweets['Days'].progress_apply(days_to_date)\n",
    "    tweets.columns=['Date', 'From', 'Tweet']\n",
    "    tweets = tweets[~tweets['From']\n",
    "        .isin(\n",
    "            [source.strip().lower()\n",
    "                for source in \n",
    "                    config['General']['exclude_sources'].split(\",\")])]\n",
    "    tweets = tweets.reset_index()\n",
    "    tweets = tweets.set_index('Date').sort_index()\n",
    "    end = time.time()\n",
    "    print (f'Data read in {end-start:.2f} seconds.\\n')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:10<00:00, 59497.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read in 12.63 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data = read_data(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    text = unicodedata.normalize('NFD', str(text)).encode('ascii', 'ignore').decode(\"utf-8\").lower()\n",
    "    return str(text)\n",
    "\n",
    "def remove_apostrophes(text):\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    #hashtags and handles\n",
    "    text = re.sub(r'\\B(\\#([0-9]|[a-zA-Z])+|\\@([0-9]|[a-zA-Z])+\\b)', '', text)\n",
    "    return text\n",
    "def remove_urls(text):\n",
    "    text= re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_numberwords(text):\n",
    "    text= re.sub(r'\\b[0-9]+\\b\\s*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(df, text_column):\n",
    "    tqdm.write(f'Cleaning up {text_column} texts...')\n",
    "    start = time.time()\n",
    "    actions = [action.strip().lower() for action in config['Text Cleaning']['actions'].split(\",\")]\n",
    "    for action in actions:\n",
    "        tqdm.write('.... ' + action)\n",
    "        df[text_column] = df[text_column].progress_apply(globals()[action])\n",
    "    end = time.time()\n",
    "    tqdm.write(f'Text cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 44190/518145 [00:00<00:02, 174830.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Tweet texts...\n",
      ".... remove_accents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:01<00:00, 287314.86it/s]\n",
      " 10%|█         | 54387/518145 [00:00<00:01, 258068.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_apostrophes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:01<00:00, 313792.25it/s]\n",
      "  2%|▏         | 8653/518145 [00:00<00:05, 86527.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_hashtags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:03<00:00, 134350.21it/s]\n",
      "  9%|▉         | 48836/518145 [00:00<00:02, 233293.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:02<00:00, 248614.07it/s]\n",
      "  3%|▎         | 15140/518145 [00:00<00:03, 151396.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_numberwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:02<00:00, 172839.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleanup finished in 12.68 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned = clean_text(input_data, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, text_column):\n",
    "    print(f'Tokenizing Dataframe[\"{text_column}\"].')\n",
    "    start = time.time()\n",
    "    df['Unigrams'] = df[text_column].progress_apply(tknzr.tokenize)\n",
    "    end = time.time()\n",
    "    print(f'Dataframe[\"{text_column}\"] tokenized in {end-start:.2f} seconds.\\n')\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stopwords(input, stops):\n",
    "    output = [i for i in input if i not in stops]\n",
    "    return output\n",
    "\n",
    "def remove_extremewords(input, min, max):\n",
    "    output = [i for i in input if (len(i)<=max and len(i)>=min)]\n",
    "    return output\n",
    "\n",
    "\n",
    "def clean_tokens(df):\n",
    "    tqdm.write('Cleaning up tokens...')\n",
    "    start = time.time()\n",
    "    tqdm.write('.... removing extreme words')\n",
    "    min = config['Text Cleaning'].getint('min_word_size') or 2\n",
    "    max = config['Text Cleaning'].getint('max_word_size') or 20\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(lambda x: remove_extremewords(x, min, max))\n",
    "    tqdm.write('.... removing stop words')\n",
    "    ll = [stopwords.words('english') + list(punctuation)] + [\"\".join(string.split()).split(',') for string in [v for k, v in config.items('Stop Words')]]\n",
    "    flat = [item for sublist in ll for item in sublist]\n",
    "    stops = set(flat)\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(lambda x: remove_stopwords(input=x, stops=stops))\n",
    "    tqdm.write('.... generating bigrams')\n",
    "    df['Bigrams'] = df['Unigrams'].progress_apply(lambda x: [f'{tuple[0]} {tuple[1]}' for tuple in list(bigrams(x))])\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Tokens cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 702/518145 [00:00<01:13, 7018.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Dataframe[\"Tweet\"].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:46<00:00, 11200.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe[\"Tweet\"] tokenized in 46.42 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenize(cleaned, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>From</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Unigrams</th>\n",
       "      <th>Bigrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>65285</td>\n",
       "      <td>@SAI</td>\n",
       "      <td>top objectively biggest tech stories of</td>\n",
       "      <td>[top, objectively, biggest, tech, stories, of]</td>\n",
       "      <td>[objectively tech, tech stories]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>195166</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>silicon valley campaign seeks startup visa for...</td>\n",
       "      <td>[silicon, valley, campaign, seeks, startup, vi...</td>\n",
       "      <td>[silicon campaign, campaign seeks, seeks start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502284</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>: my fifth annual list of the tech products i ...</td>\n",
       "      <td>[:, my, fifth, annual, list, of, the, tech, pr...</td>\n",
       "      <td>[fifth list, list tech, tech products, product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502285</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>namebench: google % project to find the fastes...</td>\n",
       "      <td>[namebench, :, google, %, project, to, find, t...</td>\n",
       "      <td>[namebench project, project find, find fastest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502286</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>six new years resolutions for apple and the ip...</td>\n",
       "      <td>[six, new, years, resolutions, for, apple, and...</td>\n",
       "      <td>[six resolutions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502287</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>ten technologies that will rock by</td>\n",
       "      <td>[ten, technologies, that, will, rock, by]</td>\n",
       "      <td>[ten technologies, technologies rock]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502288</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>hotel wifi should be a right, not a luxury  by</td>\n",
       "      <td>[hotel, wifi, should, be, a, right, ,, not, a,...</td>\n",
       "      <td>[hotel wifi, wifi right, right luxury]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502289</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>twitter and me!  why it the only social media ...</td>\n",
       "      <td>[twitter, and, me, !, why, it, the, only, soci...</td>\n",
       "      <td>[social media, media tool, tool use]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>342891</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>ben heck ps3 slim laptop pops up for sale on e...</td>\n",
       "      <td>[ben, heck, ps3, slim, laptop, pops, up, for, ...</td>\n",
       "      <td>[ben heck, heck ps3, ps3 slim, slim laptop, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>342892</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>predictions for ?</td>\n",
       "      <td>[predictions, for, ?]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>342893</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>a new year gift to engadget readers: minutes o...</td>\n",
       "      <td>[a, new, year, gift, to, engadget, readers, :,...</td>\n",
       "      <td>[engadget readers, readers minutes, minutes woz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>342894</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>us government launches distraction.gov, wants ...</td>\n",
       "      <td>[us, government, launches, distraction.gov, ,,...</td>\n",
       "      <td>[government launches, launches distraction.gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>65284</td>\n",
       "      <td>@SAI</td>\n",
       "      <td>apple expects to sell million tablets in first...</td>\n",
       "      <td>[apple, expects, to, sell, million, tablets, i...</td>\n",
       "      <td>[expects sell, sell tablets, tablets first]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>342895</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>ocz vertex pro ssd previewed: awesome, and cou...</td>\n",
       "      <td>[ocz, vertex, pro, ssd, previewed, :, awesome,...</td>\n",
       "      <td>[ocz vertex, vertex ssd, ssd previewed, previe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>342896</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>netgear ptv1000 push tv wifi video card hits t...</td>\n",
       "      <td>[netgear, ptv, 1000, push, tv, wifi, video, ca...</td>\n",
       "      <td>[netgear ptv, ptv 1000, 1000 push, push tv, tv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>182531</td>\n",
       "      <td>@BBCTech</td>\n",
       "      <td>will the evolution of smartphones see off sat-...</td>\n",
       "      <td>[will, the, evolution, of, smartphones, see, o...</td>\n",
       "      <td>[evolution smartphones, smartphones see, see s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>160763</td>\n",
       "      <td>@techreview</td>\n",
       "      <td>physics arxiv blog highlights of : september: ...</td>\n",
       "      <td>[physics, arxiv, blog, highlights, of, :, sept...</td>\n",
       "      <td>[physics arxiv, arxiv blog, blog highlights, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>195161</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>compatibility test: facebook | guy browning</td>\n",
       "      <td>[compatibility, test, :, facebook, |, guy, bro...</td>\n",
       "      <td>[compatibility test, test guy, guy browning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>195162</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>children of the virtual world</td>\n",
       "      <td>[children, of, the, virtual, world]</td>\n",
       "      <td>[children virtual, virtual world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>195163</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>a new decade: what in a name?</td>\n",
       "      <td>[a, new, decade, :, what, in, a, name, ?]</td>\n",
       "      <td>[decade name]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>195164</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>racial or religious groups could be picked out...</td>\n",
       "      <td>[racial, or, religious, groups, could, be, pic...</td>\n",
       "      <td>[racial religious, religious groups, groups pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>195165</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>the most viewed stories and galleries on techn...</td>\n",
       "      <td>[the, most, viewed, stories, and, galleries, o...</td>\n",
       "      <td>[viewed stories, stories galleries, galleries ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>11521</td>\n",
       "      <td>@ReutersTech</td>\n",
       "      <td>taiwan asustek to free up subsidiary by june -...</td>\n",
       "      <td>[taiwan, asustek, to, free, up, subsidiary, by...</td>\n",
       "      <td>[taiwan asustek, asustek subsidiary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>141317</td>\n",
       "      <td>@HuffPostTech</td>\n",
       "      <td>ultimate guide to apple tablet (or itablet or ...</td>\n",
       "      <td>[ultimate, guide, to, apple, tablet, (, or, it...</td>\n",
       "      <td>[ultimate tablet, tablet itablet, itablet isla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>195167</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>the most intriguing games of : part two</td>\n",
       "      <td>[the, most, intriguing, games, of, :, part, two]</td>\n",
       "      <td>[intriguing games, games part, part two]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>50476</td>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>ocado ipo prospects grow: the prospects of onl...</td>\n",
       "      <td>[ocado, ipo, prospects, grow, :, the, prospect...</td>\n",
       "      <td>[ocado prospects, prospects grow, grow prospec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>65282</td>\n",
       "      <td>@SAI</td>\n",
       "      <td>fox -time warner spat shows why the death of t...</td>\n",
       "      <td>[fox, -, time, warner, spat, shows, why, the, ...</td>\n",
       "      <td>[fox time, time spat, spat shows, shows death,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>50473</td>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>china mobile communications executive sacked f...</td>\n",
       "      <td>[china, mobile, communications, executive, sac...</td>\n",
       "      <td>[mobile communications, communications executi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>50474</td>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>at&amp;t severs ties with woods: at&amp;t has dropped ...</td>\n",
       "      <td>[at, &amp;, t, severs, ties, with, woods, :, at, &amp;...</td>\n",
       "      <td>[severs ties, ties dropped, dropped sponsorshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>65283</td>\n",
       "      <td>@SAI</td>\n",
       "      <td>googler running for governor of vermont</td>\n",
       "      <td>[googler, running, for, governor, of, vermont]</td>\n",
       "      <td>[googler running, running governor, governor v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455985</td>\n",
       "      <td>@verge</td>\n",
       "      <td>heres how mouse support could change the way y...</td>\n",
       "      <td>[heres, how, mouse, support, could, change, th...</td>\n",
       "      <td>[heres mouse, mouse support, support change, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>384101</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>after math: jony bye-ve</td>\n",
       "      <td>[after, math, :, jony, bye-ve]</td>\n",
       "      <td>[math bye-ve]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>384100</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>the best sous vide machine and gear</td>\n",
       "      <td>[the, best, sous, vide, machine, and, gear]</td>\n",
       "      <td>[sous vide, vide machine, machine gear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>384099</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>amd denies improperly sharing cpu tech with ch...</td>\n",
       "      <td>[amd, denies, improperly, sharing, cpu, tech, ...</td>\n",
       "      <td>[amd denies, denies improperly, improperly sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>640993</td>\n",
       "      <td>@WIRED</td>\n",
       "      <td>behold a sports car combination made in heaven...</td>\n",
       "      <td>[behold, a, sports, car, combination, made, in...</td>\n",
       "      <td>[behold sports, sports car, car combination, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>640992</td>\n",
       "      <td>@WIRED</td>\n",
       "      <td>psa: you don't actually own things like your i...</td>\n",
       "      <td>[psa, :, you, don't, actually, own, things, li...</td>\n",
       "      <td>[psa movies, movies books, books buyers, buyer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>640991</td>\n",
       "      <td>@WIRED</td>\n",
       "      <td>black holes are one of the galaxy greatest mys...</td>\n",
       "      <td>[black, holes, are, one, of, the, galaxy, grea...</td>\n",
       "      <td>[black holes, holes one, one greatest, greates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>60491</td>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>china eyes $16bn boost for tech sector with na...</td>\n",
       "      <td>[china, eyes, $, 16bn, boost, for, tech, secto...</td>\n",
       "      <td>[eyes 16bn, 16bn boost, boost tech, tech secto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>60492</td>\n",
       "      <td>@fttechnews</td>\n",
       "      <td>scientists $250m fund aims to keep start-ups i...</td>\n",
       "      <td>[scientists, $, 250m, fund, aims, to, keep, st...</td>\n",
       "      <td>[scientists 250m, 250m aims, aims keep, keep s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>139982</td>\n",
       "      <td>@futureshift</td>\n",
       "      <td>revolut lets you round up payments and donate ...</td>\n",
       "      <td>[revolut, lets, you, round, up, payments, and,...</td>\n",
       "      <td>[revolut lets, lets round, round payments, pay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>209619</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>the robots are definitely coming and will make...</td>\n",
       "      <td>[the, robots, are, definitely, coming, and, wi...</td>\n",
       "      <td>[robots definitely, definitely coming, coming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>209618</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>amazon jeff bezos pays out $38bn in divorce se...</td>\n",
       "      <td>[amazon, jeff, bezos, pays, out, $, 38bn, in, ...</td>\n",
       "      <td>[pays 38bn, 38bn divorce, divorce settlement]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>209617</td>\n",
       "      <td>@guardiantech</td>\n",
       "      <td>white house insists trump huawei reversal not ...</td>\n",
       "      <td>[white, house, insists, trump, huawei, reversa...</td>\n",
       "      <td>[white house, house insists, insists reversal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455953</td>\n",
       "      <td>@verge</td>\n",
       "      <td>apple music has surpassed million subscribers,...</td>\n",
       "      <td>[apple, music, has, surpassed, million, subscr...</td>\n",
       "      <td>[music surpassed, surpassed subscribers, subsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455954</td>\n",
       "      <td>@verge</td>\n",
       "      <td>pre-saving albums on spotify gives record labe...</td>\n",
       "      <td>[pre-saving, albums, on, spotify, gives, recor...</td>\n",
       "      <td>[pre-saving albums, albums gives, gives record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455955</td>\n",
       "      <td>@verge</td>\n",
       "      <td>is a slider phone better than a notch?</td>\n",
       "      <td>[is, a, slider, phone, better, than, a, notch, ?]</td>\n",
       "      <td>[slider phone, phone better, better notch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455956</td>\n",
       "      <td>@verge</td>\n",
       "      <td>southwest and united airlines extend max cance...</td>\n",
       "      <td>[southwest, and, united, airlines, extend, max...</td>\n",
       "      <td>[southwest united, united airlines, airlines e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455957</td>\n",
       "      <td>@verge</td>\n",
       "      <td>apple will repair macbook airs with faulty log...</td>\n",
       "      <td>[apple, will, repair, macbook, airs, with, fau...</td>\n",
       "      <td>[repair airs, airs faulty, faulty logic, logic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455958</td>\n",
       "      <td>@verge</td>\n",
       "      <td>google accused of inappropriate access to medi...</td>\n",
       "      <td>[google, accused, of, inappropriate, access, t...</td>\n",
       "      <td>[accused inappropriate, inappropriate access, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>48914</td>\n",
       "      <td>@technology</td>\n",
       "      <td>elizabeth warrens proposal to break up big tec...</td>\n",
       "      <td>[elizabeth, warrens, proposal, to, break, up, ...</td>\n",
       "      <td>[warrens proposal, proposal break, break big, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455959</td>\n",
       "      <td>@verge</td>\n",
       "      <td>displayport .is ready for 8k monitors and beyo...</td>\n",
       "      <td>[displayport, ., is, ready, for, 8k, monitors,...</td>\n",
       "      <td>[displayport ready, ready 8k, 8k monitors, mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455960</td>\n",
       "      <td>@verge</td>\n",
       "      <td>theranos founder elizabeth holmes will go on t...</td>\n",
       "      <td>[theranos, founder, elizabeth, holmes, will, g...</td>\n",
       "      <td>[theranos holmes, holmes go, go trial, trial n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455961</td>\n",
       "      <td>@verge</td>\n",
       "      <td>an amazon engineer made an ai-powered cat flap...</td>\n",
       "      <td>[an, amazon, engineer, made, an, ai-powered, c...</td>\n",
       "      <td>[engineer made, made ai-powered, ai-powered ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>645890</td>\n",
       "      <td>@DARPA</td>\n",
       "      <td>mark rosker, the director of our microsystems ...</td>\n",
       "      <td>[mark, rosker, ,, the, director, of, our, micr...</td>\n",
       "      <td>[rosker director, director microsystems, micro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>455962</td>\n",
       "      <td>@verge</td>\n",
       "      <td>play with synths in your browser with abletons...</td>\n",
       "      <td>[play, with, synths, in, your, browser, with, ...</td>\n",
       "      <td>[play synths, synths browser, browser abletons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>384087</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>spacex is still in control of all but three of...</td>\n",
       "      <td>[spacex, is, still, in, control, of, all, but,...</td>\n",
       "      <td>[still control, control three, three internet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>384086</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>countries back plan to create 'free flow' of d...</td>\n",
       "      <td>[countries, back, plan, to, create, ', free, f...</td>\n",
       "      <td>[countries back, back plan, plan create, creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>384085</td>\n",
       "      <td>@engadget</td>\n",
       "      <td>the new raspberry pi is ready for 4k video</td>\n",
       "      <td>[the, new, raspberry, pi, is, ready, for, 4k, ...</td>\n",
       "      <td>[raspberry pi, pi ready, ready 4k, 4k video]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>640995</td>\n",
       "      <td>@WIRED</td>\n",
       "      <td>this is what san francisco looked like before ...</td>\n",
       "      <td>[this, is, what, san, francisco, looked, like,...</td>\n",
       "      <td>[looked electric, electric scooters, scooters ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>179794</td>\n",
       "      <td>@techreview</td>\n",
       "      <td>fifty years after neil armstrong stepped onto ...</td>\n",
       "      <td>[fifty, years, after, neil, armstrong, stepped...</td>\n",
       "      <td>[fifty armstrong, armstrong stepped, stepped o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518145 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index           From  \\\n",
       "Date                                \n",
       "2010-01-01   65285           @SAI   \n",
       "2010-01-01  195166  @guardiantech   \n",
       "2010-01-01  502284    @TechCrunch   \n",
       "2010-01-01  502285    @TechCrunch   \n",
       "2010-01-01  502286    @TechCrunch   \n",
       "2010-01-01  502287    @TechCrunch   \n",
       "2010-01-01  502288    @TechCrunch   \n",
       "2010-01-01  502289    @TechCrunch   \n",
       "2010-01-01  342891      @engadget   \n",
       "2010-01-01  342892      @engadget   \n",
       "2010-01-01  342893      @engadget   \n",
       "2010-01-01  342894      @engadget   \n",
       "2010-01-01   65284           @SAI   \n",
       "2010-01-01  342895      @engadget   \n",
       "2010-01-01  342896      @engadget   \n",
       "2010-01-01  182531       @BBCTech   \n",
       "2010-01-01  160763    @techreview   \n",
       "2010-01-01  195161  @guardiantech   \n",
       "2010-01-01  195162  @guardiantech   \n",
       "2010-01-01  195163  @guardiantech   \n",
       "2010-01-01  195164  @guardiantech   \n",
       "2010-01-01  195165  @guardiantech   \n",
       "2010-01-01   11521   @ReutersTech   \n",
       "2010-01-01  141317  @HuffPostTech   \n",
       "2010-01-01  195167  @guardiantech   \n",
       "2010-01-01   50476    @fttechnews   \n",
       "2010-01-01   65282           @SAI   \n",
       "2010-01-01   50473    @fttechnews   \n",
       "2010-01-01   50474    @fttechnews   \n",
       "2010-01-01   65283           @SAI   \n",
       "...            ...            ...   \n",
       "2019-06-30  455985         @verge   \n",
       "2019-06-30  384101      @engadget   \n",
       "2019-06-30  384100      @engadget   \n",
       "2019-06-30  384099      @engadget   \n",
       "2019-06-30  640993         @WIRED   \n",
       "2019-06-30  640992         @WIRED   \n",
       "2019-06-30  640991         @WIRED   \n",
       "2019-06-30   60491    @fttechnews   \n",
       "2019-06-30   60492    @fttechnews   \n",
       "2019-06-30  139982   @futureshift   \n",
       "2019-06-30  209619  @guardiantech   \n",
       "2019-06-30  209618  @guardiantech   \n",
       "2019-06-30  209617  @guardiantech   \n",
       "2019-06-30  455953         @verge   \n",
       "2019-06-30  455954         @verge   \n",
       "2019-06-30  455955         @verge   \n",
       "2019-06-30  455956         @verge   \n",
       "2019-06-30  455957         @verge   \n",
       "2019-06-30  455958         @verge   \n",
       "2019-06-30   48914    @technology   \n",
       "2019-06-30  455959         @verge   \n",
       "2019-06-30  455960         @verge   \n",
       "2019-06-30  455961         @verge   \n",
       "2019-06-30  645890         @DARPA   \n",
       "2019-06-30  455962         @verge   \n",
       "2019-06-30  384087      @engadget   \n",
       "2019-06-30  384086      @engadget   \n",
       "2019-06-30  384085      @engadget   \n",
       "2019-06-30  640995         @WIRED   \n",
       "2019-06-30  179794    @techreview   \n",
       "\n",
       "                                                        Tweet  \\\n",
       "Date                                                            \n",
       "2010-01-01           top objectively biggest tech stories of    \n",
       "2010-01-01  silicon valley campaign seeks startup visa for...   \n",
       "2010-01-01  : my fifth annual list of the tech products i ...   \n",
       "2010-01-01  namebench: google % project to find the fastes...   \n",
       "2010-01-01  six new years resolutions for apple and the ip...   \n",
       "2010-01-01                ten technologies that will rock by    \n",
       "2010-01-01    hotel wifi should be a right, not a luxury  by    \n",
       "2010-01-01  twitter and me!  why it the only social media ...   \n",
       "2010-01-01  ben heck ps3 slim laptop pops up for sale on e...   \n",
       "2010-01-01                                 predictions for ?    \n",
       "2010-01-01  a new year gift to engadget readers: minutes o...   \n",
       "2010-01-01  us government launches distraction.gov, wants ...   \n",
       "2010-01-01  apple expects to sell million tablets in first...   \n",
       "2010-01-01  ocz vertex pro ssd previewed: awesome, and cou...   \n",
       "2010-01-01  netgear ptv1000 push tv wifi video card hits t...   \n",
       "2010-01-01  will the evolution of smartphones see off sat-...   \n",
       "2010-01-01  physics arxiv blog highlights of : september: ...   \n",
       "2010-01-01       compatibility test: facebook | guy browning    \n",
       "2010-01-01                     children of the virtual world    \n",
       "2010-01-01                     a new decade: what in a name?    \n",
       "2010-01-01  racial or religious groups could be picked out...   \n",
       "2010-01-01  the most viewed stories and galleries on techn...   \n",
       "2010-01-01  taiwan asustek to free up subsidiary by june -...   \n",
       "2010-01-01  ultimate guide to apple tablet (or itablet or ...   \n",
       "2010-01-01           the most intriguing games of : part two    \n",
       "2010-01-01  ocado ipo prospects grow: the prospects of onl...   \n",
       "2010-01-01  fox -time warner spat shows why the death of t...   \n",
       "2010-01-01  china mobile communications executive sacked f...   \n",
       "2010-01-01  at&t severs ties with woods: at&t has dropped ...   \n",
       "2010-01-01           googler running for governor of vermont    \n",
       "...                                                       ...   \n",
       "2019-06-30  heres how mouse support could change the way y...   \n",
       "2019-06-30                          after math: jony bye-ve     \n",
       "2019-06-30              the best sous vide machine and gear     \n",
       "2019-06-30  amd denies improperly sharing cpu tech with ch...   \n",
       "2019-06-30  behold a sports car combination made in heaven...   \n",
       "2019-06-30  psa: you don't actually own things like your i...   \n",
       "2019-06-30  black holes are one of the galaxy greatest mys...   \n",
       "2019-06-30  china eyes $16bn boost for tech sector with na...   \n",
       "2019-06-30  scientists $250m fund aims to keep start-ups i...   \n",
       "2019-06-30  revolut lets you round up payments and donate ...   \n",
       "2019-06-30  the robots are definitely coming and will make...   \n",
       "2019-06-30  amazon jeff bezos pays out $38bn in divorce se...   \n",
       "2019-06-30  white house insists trump huawei reversal not ...   \n",
       "2019-06-30  apple music has surpassed million subscribers,...   \n",
       "2019-06-30  pre-saving albums on spotify gives record labe...   \n",
       "2019-06-30             is a slider phone better than a notch?   \n",
       "2019-06-30  southwest and united airlines extend max cance...   \n",
       "2019-06-30  apple will repair macbook airs with faulty log...   \n",
       "2019-06-30  google accused of inappropriate access to medi...   \n",
       "2019-06-30  elizabeth warrens proposal to break up big tec...   \n",
       "2019-06-30  displayport .is ready for 8k monitors and beyo...   \n",
       "2019-06-30  theranos founder elizabeth holmes will go on t...   \n",
       "2019-06-30  an amazon engineer made an ai-powered cat flap...   \n",
       "2019-06-30  mark rosker, the director of our microsystems ...   \n",
       "2019-06-30  play with synths in your browser with abletons...   \n",
       "2019-06-30  spacex is still in control of all but three of...   \n",
       "2019-06-30  countries back plan to create 'free flow' of d...   \n",
       "2019-06-30         the new raspberry pi is ready for 4k video   \n",
       "2019-06-30  this is what san francisco looked like before ...   \n",
       "2019-06-30  fifty years after neil armstrong stepped onto ...   \n",
       "\n",
       "                                                     Unigrams  \\\n",
       "Date                                                            \n",
       "2010-01-01     [top, objectively, biggest, tech, stories, of]   \n",
       "2010-01-01  [silicon, valley, campaign, seeks, startup, vi...   \n",
       "2010-01-01  [:, my, fifth, annual, list, of, the, tech, pr...   \n",
       "2010-01-01  [namebench, :, google, %, project, to, find, t...   \n",
       "2010-01-01  [six, new, years, resolutions, for, apple, and...   \n",
       "2010-01-01          [ten, technologies, that, will, rock, by]   \n",
       "2010-01-01  [hotel, wifi, should, be, a, right, ,, not, a,...   \n",
       "2010-01-01  [twitter, and, me, !, why, it, the, only, soci...   \n",
       "2010-01-01  [ben, heck, ps3, slim, laptop, pops, up, for, ...   \n",
       "2010-01-01                              [predictions, for, ?]   \n",
       "2010-01-01  [a, new, year, gift, to, engadget, readers, :,...   \n",
       "2010-01-01  [us, government, launches, distraction.gov, ,,...   \n",
       "2010-01-01  [apple, expects, to, sell, million, tablets, i...   \n",
       "2010-01-01  [ocz, vertex, pro, ssd, previewed, :, awesome,...   \n",
       "2010-01-01  [netgear, ptv, 1000, push, tv, wifi, video, ca...   \n",
       "2010-01-01  [will, the, evolution, of, smartphones, see, o...   \n",
       "2010-01-01  [physics, arxiv, blog, highlights, of, :, sept...   \n",
       "2010-01-01  [compatibility, test, :, facebook, |, guy, bro...   \n",
       "2010-01-01                [children, of, the, virtual, world]   \n",
       "2010-01-01          [a, new, decade, :, what, in, a, name, ?]   \n",
       "2010-01-01  [racial, or, religious, groups, could, be, pic...   \n",
       "2010-01-01  [the, most, viewed, stories, and, galleries, o...   \n",
       "2010-01-01  [taiwan, asustek, to, free, up, subsidiary, by...   \n",
       "2010-01-01  [ultimate, guide, to, apple, tablet, (, or, it...   \n",
       "2010-01-01   [the, most, intriguing, games, of, :, part, two]   \n",
       "2010-01-01  [ocado, ipo, prospects, grow, :, the, prospect...   \n",
       "2010-01-01  [fox, -, time, warner, spat, shows, why, the, ...   \n",
       "2010-01-01  [china, mobile, communications, executive, sac...   \n",
       "2010-01-01  [at, &, t, severs, ties, with, woods, :, at, &...   \n",
       "2010-01-01     [googler, running, for, governor, of, vermont]   \n",
       "...                                                       ...   \n",
       "2019-06-30  [heres, how, mouse, support, could, change, th...   \n",
       "2019-06-30                     [after, math, :, jony, bye-ve]   \n",
       "2019-06-30        [the, best, sous, vide, machine, and, gear]   \n",
       "2019-06-30  [amd, denies, improperly, sharing, cpu, tech, ...   \n",
       "2019-06-30  [behold, a, sports, car, combination, made, in...   \n",
       "2019-06-30  [psa, :, you, don't, actually, own, things, li...   \n",
       "2019-06-30  [black, holes, are, one, of, the, galaxy, grea...   \n",
       "2019-06-30  [china, eyes, $, 16bn, boost, for, tech, secto...   \n",
       "2019-06-30  [scientists, $, 250m, fund, aims, to, keep, st...   \n",
       "2019-06-30  [revolut, lets, you, round, up, payments, and,...   \n",
       "2019-06-30  [the, robots, are, definitely, coming, and, wi...   \n",
       "2019-06-30  [amazon, jeff, bezos, pays, out, $, 38bn, in, ...   \n",
       "2019-06-30  [white, house, insists, trump, huawei, reversa...   \n",
       "2019-06-30  [apple, music, has, surpassed, million, subscr...   \n",
       "2019-06-30  [pre-saving, albums, on, spotify, gives, recor...   \n",
       "2019-06-30  [is, a, slider, phone, better, than, a, notch, ?]   \n",
       "2019-06-30  [southwest, and, united, airlines, extend, max...   \n",
       "2019-06-30  [apple, will, repair, macbook, airs, with, fau...   \n",
       "2019-06-30  [google, accused, of, inappropriate, access, t...   \n",
       "2019-06-30  [elizabeth, warrens, proposal, to, break, up, ...   \n",
       "2019-06-30  [displayport, ., is, ready, for, 8k, monitors,...   \n",
       "2019-06-30  [theranos, founder, elizabeth, holmes, will, g...   \n",
       "2019-06-30  [an, amazon, engineer, made, an, ai-powered, c...   \n",
       "2019-06-30  [mark, rosker, ,, the, director, of, our, micr...   \n",
       "2019-06-30  [play, with, synths, in, your, browser, with, ...   \n",
       "2019-06-30  [spacex, is, still, in, control, of, all, but,...   \n",
       "2019-06-30  [countries, back, plan, to, create, ', free, f...   \n",
       "2019-06-30  [the, new, raspberry, pi, is, ready, for, 4k, ...   \n",
       "2019-06-30  [this, is, what, san, francisco, looked, like,...   \n",
       "2019-06-30  [fifty, years, after, neil, armstrong, stepped...   \n",
       "\n",
       "                                                      Bigrams  \n",
       "Date                                                           \n",
       "2010-01-01                   [objectively tech, tech stories]  \n",
       "2010-01-01  [silicon campaign, campaign seeks, seeks start...  \n",
       "2010-01-01  [fifth list, list tech, tech products, product...  \n",
       "2010-01-01  [namebench project, project find, find fastest...  \n",
       "2010-01-01                                  [six resolutions]  \n",
       "2010-01-01              [ten technologies, technologies rock]  \n",
       "2010-01-01             [hotel wifi, wifi right, right luxury]  \n",
       "2010-01-01               [social media, media tool, tool use]  \n",
       "2010-01-01  [ben heck, heck ps3, ps3 slim, slim laptop, la...  \n",
       "2010-01-01                                                 []  \n",
       "2010-01-01   [engadget readers, readers minutes, minutes woz]  \n",
       "2010-01-01  [government launches, launches distraction.gov...  \n",
       "2010-01-01        [expects sell, sell tablets, tablets first]  \n",
       "2010-01-01  [ocz vertex, vertex ssd, ssd previewed, previe...  \n",
       "2010-01-01  [netgear ptv, ptv 1000, 1000 push, push tv, tv...  \n",
       "2010-01-01  [evolution smartphones, smartphones see, see s...  \n",
       "2010-01-01  [physics arxiv, arxiv blog, blog highlights, h...  \n",
       "2010-01-01       [compatibility test, test guy, guy browning]  \n",
       "2010-01-01                  [children virtual, virtual world]  \n",
       "2010-01-01                                      [decade name]  \n",
       "2010-01-01  [racial religious, religious groups, groups pi...  \n",
       "2010-01-01  [viewed stories, stories galleries, galleries ...  \n",
       "2010-01-01               [taiwan asustek, asustek subsidiary]  \n",
       "2010-01-01  [ultimate tablet, tablet itablet, itablet isla...  \n",
       "2010-01-01           [intriguing games, games part, part two]  \n",
       "2010-01-01  [ocado prospects, prospects grow, grow prospec...  \n",
       "2010-01-01  [fox time, time spat, spat shows, shows death,...  \n",
       "2010-01-01  [mobile communications, communications executi...  \n",
       "2010-01-01  [severs ties, ties dropped, dropped sponsorshi...  \n",
       "2010-01-01  [googler running, running governor, governor v...  \n",
       "...                                                       ...  \n",
       "2019-06-30  [heres mouse, mouse support, support change, c...  \n",
       "2019-06-30                                      [math bye-ve]  \n",
       "2019-06-30            [sous vide, vide machine, machine gear]  \n",
       "2019-06-30  [amd denies, denies improperly, improperly sha...  \n",
       "2019-06-30  [behold sports, sports car, car combination, c...  \n",
       "2019-06-30  [psa movies, movies books, books buyers, buyer...  \n",
       "2019-06-30  [black holes, holes one, one greatest, greates...  \n",
       "2019-06-30  [eyes 16bn, 16bn boost, boost tech, tech secto...  \n",
       "2019-06-30  [scientists 250m, 250m aims, aims keep, keep s...  \n",
       "2019-06-30  [revolut lets, lets round, round payments, pay...  \n",
       "2019-06-30  [robots definitely, definitely coming, coming ...  \n",
       "2019-06-30      [pays 38bn, 38bn divorce, divorce settlement]  \n",
       "2019-06-30  [white house, house insists, insists reversal,...  \n",
       "2019-06-30  [music surpassed, surpassed subscribers, subsc...  \n",
       "2019-06-30  [pre-saving albums, albums gives, gives record...  \n",
       "2019-06-30         [slider phone, phone better, better notch]  \n",
       "2019-06-30  [southwest united, united airlines, airlines e...  \n",
       "2019-06-30  [repair airs, airs faulty, faulty logic, logic...  \n",
       "2019-06-30  [accused inappropriate, inappropriate access, ...  \n",
       "2019-06-30  [warrens proposal, proposal break, break big, ...  \n",
       "2019-06-30  [displayport ready, ready 8k, 8k monitors, mon...  \n",
       "2019-06-30  [theranos holmes, holmes go, go trial, trial n...  \n",
       "2019-06-30  [engineer made, made ai-powered, ai-powered ca...  \n",
       "2019-06-30  [rosker director, director microsystems, micro...  \n",
       "2019-06-30  [play synths, synths browser, browser abletons...  \n",
       "2019-06-30  [still control, control three, three internet,...  \n",
       "2019-06-30  [countries back, back plan, plan create, creat...  \n",
       "2019-06-30       [raspberry pi, pi ready, ready 4k, 4k video]  \n",
       "2019-06-30  [looked electric, electric scooters, scooters ...  \n",
       "2019-06-30  [fifty armstrong, armstrong stepped, stepped o...  \n",
       "\n",
       "[518145 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9023/518145 [00:00<00:05, 88132.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tokens...\n",
      ".... removing extreme words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:03<00:00, 146924.72it/s]\n",
      "  8%|▊         | 43987/518145 [00:00<00:02, 204880.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:02<00:00, 180325.95it/s]\n",
      "  5%|▍         | 25065/518145 [00:00<00:04, 118792.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... generating bigrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518145/518145 [00:05<00:00, 89224.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens cleanup finished in 12.78 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens(tokenized).to_pickle(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
