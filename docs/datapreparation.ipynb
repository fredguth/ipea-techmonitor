{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from string import punctuation\n",
    "from configparser import ConfigParser, ExtendedInterpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    start = time.time()\n",
    "    tqdm.pandas()\n",
    "    print ('Running setup...')\n",
    "    resources = ['taggers/averaged_perceptron_tagger', 'corpora/wordnet', 'corpora/stopwords', 'tokenizers/punkt']\n",
    "    for path in resources:\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            nltk.download(path.split('/')[1])\n",
    "    end = time.time()\n",
    "    print (f'Setup finished in {end-start:.2f} seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup...\n",
      "Setup finished in 0.00 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_date(srl_no, first=datetime.datetime(2010,1,1,0,0)):\n",
    "    days = int(srl_no-1)\n",
    "    new_date = first + datetime.timedelta(days)\n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    print('Reading data...')\n",
    "    start = time.time()\n",
    "    tweets = pd.read_csv(filename, encoding='latin-1', sep=';',header=0, names=['StringDate', 'Days', 'From', 'Tweet'])\n",
    "    tweets = tweets.filter(items=['Days', 'From', 'Tweet'])\n",
    "    tweets['Days'] = tweets['Days'].progress_apply(days_to_date)\n",
    "    tweets.columns=['Date', 'From', 'Tweet']\n",
    "    tweets = tweets.reset_index()\n",
    "    tweets = tweets.set_index('Date').sort_index()\n",
    "    end = time.time()\n",
    "    print (f'Data read in {end-start:.2f} seconds.\\n')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:01<00:00, 364901.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read in 3.55 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = '../data/twitter.csv'\n",
    "data = read_data(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>From</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>65282</td>\n",
       "      <td>@SAI</td>\n",
       "      <td>FOX -Time Warner Spat Shows Why The Death Of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502287</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>Ten Technologies That Will Rock 2010 http://tc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502288</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>Hotel WiFi Should Be a Right, Not a Luxury htt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index         From  \\\n",
       "Date                              \n",
       "2010-01-01   65282         @SAI   \n",
       "2010-01-01  502287  @TechCrunch   \n",
       "2010-01-01  502288  @TechCrunch   \n",
       "\n",
       "                                                        Tweet  \n",
       "Date                                                           \n",
       "2010-01-01  FOX -Time Warner Spat Shows Why The Death Of T...  \n",
       "2010-01-01  Ten Technologies That Will Rock 2010 http://tc...  \n",
       "2010-01-01  Hotel WiFi Should Be a Right, Not a Luxury htt...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['From'] = data['From'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>From</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>65282</td>\n",
       "      <td>@sai</td>\n",
       "      <td>FOX -Time Warner Spat Shows Why The Death Of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502287</td>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>Ten Technologies That Will Rock 2010 http://tc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502288</td>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>Hotel WiFi Should Be a Right, Not a Luxury htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502289</td>\n",
       "      <td>@techcrunch</td>\n",
       "      <td>Twitter and Me!  Why It's The Only Social Medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>160763</td>\n",
       "      <td>@techreview</td>\n",
       "      <td>Physics arXiv Blog's Highlights of 2009: Septe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index         From  \\\n",
       "Date                              \n",
       "2010-01-01   65282         @sai   \n",
       "2010-01-01  502287  @techcrunch   \n",
       "2010-01-01  502288  @techcrunch   \n",
       "2010-01-01  502289  @techcrunch   \n",
       "2010-01-01  160763  @techreview   \n",
       "\n",
       "                                                        Tweet  \n",
       "Date                                                           \n",
       "2010-01-01  FOX -Time Warner Spat Shows Why The Death Of T...  \n",
       "2010-01-01  Ten Technologies That Will Rock 2010 http://tc...  \n",
       "2010-01-01  Hotel WiFi Should Be a Right, Not a Luxury htt...  \n",
       "2010-01-01  Twitter and Me!  Why It's The Only Social Medi...  \n",
       "2010-01-01  Physics arXiv Blog's Highlights of 2009: Septe...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>From</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>160763</td>\n",
       "      <td>@techreview</td>\n",
       "      <td>Physics arXiv Blog's Highlights of 2009: Septe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>213682</td>\n",
       "      <td>@mashable</td>\n",
       "      <td>One Website Does Your Resolutions for You ",
       " Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>213683</td>\n",
       "      <td>@mashable</td>\n",
       "      <td>In 2010, Your iPhone Could Be a Credit Card Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>213684</td>\n",
       "      <td>@mashable</td>\n",
       "      <td>10 Easy Ways to Green Your Web Site - http://b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>213685</td>\n",
       "      <td>@mashable</td>\n",
       "      <td>PayPal vs Fake PayPal: Can You Tell the Differ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index         From  \\\n",
       "Date                              \n",
       "2010-01-01  160763  @techreview   \n",
       "2010-01-01  213682    @mashable   \n",
       "2010-01-01  213683    @mashable   \n",
       "2010-01-01  213684    @mashable   \n",
       "2010-01-01  213685    @mashable   \n",
       "\n",
       "                                                        Tweet  \n",
       "Date                                                           \n",
       "2010-01-01  Physics arXiv Blog's Highlights of 2009: Septe...  \n",
       "2010-01-01  One Website Does Your Resolutions for You \n",
       " Al...  \n",
       "2010-01-01  In 2010, Your iPhone Could Be a Credit Card Re...  \n",
       "2010-01-01  10 Easy Ways to Green Your Web Site - http://b...  \n",
       "2010-01-01  PayPal vs Fake PayPal: Can You Tell the Differ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[~data['From'].isin([source.lower() for source in ['@SAI', '@TechCrunch']])]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config.ini']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser(\n",
    "    inline_comment_prefixes=\"#;\",\n",
    "    interpolation=ExtendedInterpolation())\n",
    "config.read('../config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type ConfigParser is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f8d686a46921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type ConfigParser is not JSON serializable"
     ]
    }
   ],
   "source": [
    "json.dumps(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.ini', 'w') as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data['From']\n",
    "        .isin(\n",
    "            [source.strip().lower()\n",
    "                for source in \n",
    "                    config['General']['exclude_sources'].split(\",\")])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    text = unicodedata.normalize('NFD', str(text)).encode('ascii', 'ignore').decode(\"utf-8\").lower()\n",
    "    return str(text)\n",
    "\n",
    "def remove_apostrophes(text):\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    #hashtags and handles\n",
    "    text = re.sub(r'\\B(\\#[a-zA-Z]+|\\@[a-zA-Z]+\\b)', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    text= re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_numberwords(text):\n",
    "    text= re.sub(r'\\b[0-9]+\\b\\s*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, text_column):\n",
    "    tqdm.write(f'Cleaning up {text_column} texts...')\n",
    "    start = time.time()\n",
    "    actions = [action.strip().lower() for action in config['Text Cleaning']['actions'].split(\",\")]\n",
    "    for action in actions:\n",
    "        tqdm.write('.... ' + action)\n",
    "        df[text_column] = df[text_column].progress_apply(globals()[action])\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Text cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [action.strip().lower() for action in config['Text Cleaning']['actions'].split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['remove_accents',\n",
       " 'remove_apostrophes',\n",
       " 'remove_hashtags',\n",
       " 'remove_numberwords',\n",
       " 'remove_stopwords',\n",
       " 'save_tokenized']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 48428/382045 [00:00<00:01, 197390.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Tweet texts...\n",
      ".... remove_accents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382045/382045 [00:01<00:00, 295400.69it/s]\n",
      " 15%|█▌        | 58446/382045 [00:00<00:01, 274704.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_apostrophes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382045/382045 [00:01<00:00, 318650.21it/s]\n",
      "  8%|▊         | 30740/382045 [00:00<00:02, 149178.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... remove_hashtags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 358959/382045 [00:02<00:00, 132090.89it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-793a33d28656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tweet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-d34a12cc751d>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(df, text_column)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.... '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Text cleanup finished in {end-start:.2f} seconds.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;31m# Close bar and return pandas calculation result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-3c470cd60321>\u001b[0m in \u001b[0;36mremove_hashtags\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#hashtags and handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\B(\\#[a-zA-Z]+|\\@[a-zA-Z]+\\b)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cleaned = clean_text(data, 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned.to_pickle('cleaned.data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(text):\n",
    "    new_text = []\n",
    "    for word, tag in pos_tag(tknzr.tokenize(text)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 's'] else None\n",
    "        if wntag:  # remove verbs\n",
    "            lemma = lmtzr.lemmatize(word, wntag)\n",
    "            new_text.append(lemma)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def tokenize(df, text_column):\n",
    "    print(f'Tokenizing Dataframe[\"{text_column}\"].')\n",
    "    start = time.time()\n",
    "    #df['Unigrams'] = df[text_column].progress_apply(_tokenize)\n",
    "    df['Unigrams'] = df[text_column].progress_apply(tknzr.tokenize)\n",
    "    end = time.time()\n",
    "    print(f'Dataframe[\"{text_column}\"] tokenized in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 317/645981 [00:00<03:32, 3038.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Dataframe[\"Tweet\"].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:53<00:00, 12147.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe[\"Tweet\"] tokenized in 53.44 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens = tokenize(cleaned, 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation) + [' ', 'rt', '...', '-->', ']:', '}:'])\n",
    "    output = [i for i in input if i not in stop_words]\n",
    "    return output\n",
    "\n",
    "def remove_extremewords(input):\n",
    "    output = [i for i in input if (len(i)<20 and len(i)>1)]   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(df):\n",
    "    tqdm.write('Cleaning up tokens...')\n",
    "    start = time.time()\n",
    "    tqdm.write('.... removing stop words')\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(remove_stopwords)\n",
    "    tqdm.write('.... removing extreme words')\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(remove_extremewords)\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Tokens cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 649/645981 [00:00<04:44, 2265.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tokens...\n",
      ".... removing stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [02:06<00:00, 5119.29it/s]\n",
      "  3%|▎         | 19606/645981 [00:00<00:03, 195979.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing extreme words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:03<00:00, 210588.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens cleanup finished in 129.56 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleantokens = clean_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops= set(stopwords.words('english') +\n",
    "    [\"\".join(string.split()).split(',') \n",
    "        for string in [v for k, v in config.items('Stop Words')]])\n",
    "    output = [i for i in input if i not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[]\n",
    "[l.append(list) for list in [\"\".join(string.split()).split(',') \n",
    "        for string in [v for k, v in config.items('Stop Words')]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in (stopwords.words('english') + [\"\".join(string.split()).split(',') \n",
    "        for string in [v for k, v in config.items('Stop Words')]]) for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [stopwords.words('english')] + [\"\".join(string.split()).split(',') \n",
    "        for string in [v for k, v in config.items('Stop Words')]]\n",
    "l = [item for sublist in ll for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'cause\",\n",
       " '):',\n",
       " '-->',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '/:',\n",
       " '4s',\n",
       " '5s',\n",
       " '6s',\n",
       " ':p',\n",
       " '<u+>',\n",
       " \"Ha'ta\",\n",
       " \"I'd\",\n",
       " \"I'll\",\n",
       " \"I'm\",\n",
       " \"I'm'a\",\n",
       " \"I'm'o\",\n",
       " \"I've\",\n",
       " \"S'e\",\n",
       " ']:',\n",
       " 'a',\n",
       " 'aapl',\n",
       " 'aaron',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'acquisition',\n",
       " 'actually',\n",
       " 'adele',\n",
       " 'adobe',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ahmed',\n",
       " 'ain',\n",
       " \"ain't\",\n",
       " 'airbnb',\n",
       " 'alan',\n",
       " 'alec',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alibaba',\n",
       " 'alicia',\n",
       " 'all',\n",
       " 'already',\n",
       " 'also',\n",
       " 'am',\n",
       " 'amazon',\n",
       " 'amazons',\n",
       " 'america',\n",
       " 'american',\n",
       " \"amn't\",\n",
       " 'an',\n",
       " 'and',\n",
       " 'andreesen',\n",
       " 'andrew',\n",
       " 'android',\n",
       " 'andy',\n",
       " 'angeles',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'annual',\n",
       " 'another',\n",
       " 'answers',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'anytime',\n",
       " 'aol',\n",
       " 'appl',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " 'around',\n",
       " 'arrive',\n",
       " 'arrives',\n",
       " 'as',\n",
       " 'ashley',\n",
       " 'ashton',\n",
       " 'ask',\n",
       " 'asks',\n",
       " 'assange',\n",
       " 'asus',\n",
       " 'at',\n",
       " 'atari',\n",
       " 'audi',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'available',\n",
       " 'avengers',\n",
       " 'awards',\n",
       " 'aws',\n",
       " 'azure',\n",
       " 'ballmer',\n",
       " 'batman',\n",
       " 'bbc',\n",
       " 'be',\n",
       " 'beatles',\n",
       " 'because',\n",
       " 'become',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'berlin',\n",
       " 'bernie',\n",
       " 'best',\n",
       " 'between',\n",
       " 'bezos',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'billionaire',\n",
       " 'billions',\n",
       " 'birthday',\n",
       " 'blackberry',\n",
       " 'bloomberg',\n",
       " 'bloomberg.com',\n",
       " 'bmw',\n",
       " 'bob',\n",
       " 'boeing',\n",
       " 'boston',\n",
       " 'both',\n",
       " 'bowl',\n",
       " 'brazil',\n",
       " 'briefing',\n",
       " 'bring',\n",
       " 'brings',\n",
       " 'british',\n",
       " 'build',\n",
       " 'building',\n",
       " 'builds',\n",
       " 'built',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'buys',\n",
       " 'by',\n",
       " \"cain't\",\n",
       " 'california',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'canada',\n",
       " 'canon',\n",
       " 'cant',\n",
       " 'carol',\n",
       " 'ceo',\n",
       " 'ces',\n",
       " 'charlie',\n",
       " 'chelsea',\n",
       " 'chicago',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chris',\n",
       " 'christmas',\n",
       " 'chrome',\n",
       " 'chromebook',\n",
       " 'chromecast',\n",
       " 'clinton',\n",
       " 'co-founder',\n",
       " 'content',\n",
       " 'could',\n",
       " \"could've\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " \"couldn't've\",\n",
       " 'craig',\n",
       " 'csco',\n",
       " 'cup',\n",
       " 'd',\n",
       " 'daily',\n",
       " \"daren't\",\n",
       " \"daresn't\",\n",
       " 'darth',\n",
       " \"dasn't\",\n",
       " 'david',\n",
       " 'day',\n",
       " 'deals',\n",
       " 'december',\n",
       " 'dell',\n",
       " 'dennis',\n",
       " 'devin',\n",
       " 'dick',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'dislike',\n",
       " 'disney',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'donald',\n",
       " 'dont',\n",
       " 'dorsey',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downloading',\n",
       " 'dre',\n",
       " 'dropbox',\n",
       " 'during',\n",
       " \"e'er\",\n",
       " 'each',\n",
       " 'earnings',\n",
       " 'earthquake',\n",
       " 'ed',\n",
       " 'eduardo',\n",
       " 'edward',\n",
       " 'egypt',\n",
       " 'election',\n",
       " 'elections',\n",
       " 'elizabeth',\n",
       " 'ellen',\n",
       " 'elon',\n",
       " 'else',\n",
       " 'employee',\n",
       " 'employees',\n",
       " 'england',\n",
       " 'episode',\n",
       " 'equifax',\n",
       " 'eric',\n",
       " 'ericson',\n",
       " 'ericsson',\n",
       " 'eu',\n",
       " 'european',\n",
       " 'ev',\n",
       " 'evan',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'every',\n",
       " \"everyone's\",\n",
       " 'example',\n",
       " 'examples',\n",
       " 'exponentially',\n",
       " 'facebook',\n",
       " 'facebooks',\n",
       " 'fb',\n",
       " 'feb',\n",
       " 'february',\n",
       " 'felix',\n",
       " 'ferrari',\n",
       " 'few',\n",
       " 'finna',\n",
       " 'firefox',\n",
       " 'for',\n",
       " 'forbes',\n",
       " 'forbes.com',\n",
       " 'ford',\n",
       " 'fortnite',\n",
       " 'founder',\n",
       " 'foursquare',\n",
       " 'foxconn',\n",
       " 'france',\n",
       " 'francisco',\n",
       " 'free',\n",
       " 'friday',\n",
       " 'from',\n",
       " 'fund',\n",
       " 'funding',\n",
       " 'further',\n",
       " 'gaga',\n",
       " 'galaxy',\n",
       " 'gates',\n",
       " 'george',\n",
       " 'germany',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'gift',\n",
       " 'gillmor',\n",
       " 'gimme',\n",
       " 'ginni',\n",
       " \"giv'n\",\n",
       " 'gizmodo',\n",
       " 'gmail',\n",
       " 'godaddy',\n",
       " 'goes',\n",
       " 'goetz',\n",
       " \"gon't\",\n",
       " 'gonna',\n",
       " 'goog',\n",
       " 'google',\n",
       " 'googles',\n",
       " 'gopro',\n",
       " 'gotta',\n",
       " 'greek',\n",
       " 'groupon',\n",
       " 'guide',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'haiti',\n",
       " 'harry',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'hbo',\n",
       " 'hbos',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " \"he've\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hilary',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hottest',\n",
       " 'houston',\n",
       " 'how',\n",
       " \"how'd\",\n",
       " \"how'll\",\n",
       " \"how're\",\n",
       " \"how's\",\n",
       " 'howdy',\n",
       " 'hp',\n",
       " 'htc',\n",
       " 'huawei',\n",
       " 'huffington',\n",
       " 'huffpo',\n",
       " 'hugo',\n",
       " 'hulu',\n",
       " 'hurricane',\n",
       " 'i',\n",
       " 'ibm',\n",
       " 'ibooks',\n",
       " 'icloud',\n",
       " 'if',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'ikea',\n",
       " 'imac',\n",
       " 'important',\n",
       " 'in',\n",
       " 'insider',\n",
       " 'instagram',\n",
       " 'intel',\n",
       " 'into',\n",
       " 'investment',\n",
       " 'investor',\n",
       " 'ios',\n",
       " 'ipad',\n",
       " 'ipads',\n",
       " 'iphone',\n",
       " 'ipo',\n",
       " 'ipod',\n",
       " 'ireland',\n",
       " 'irish',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'isnt',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'italia',\n",
       " 'italy',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'itunes',\n",
       " 'iwatch',\n",
       " 'jack',\n",
       " 'james',\n",
       " 'jan',\n",
       " 'january',\n",
       " 'japan',\n",
       " 'jedi',\n",
       " 'jeff',\n",
       " 'jerry',\n",
       " 'jk',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'johnatan',\n",
       " 'jon',\n",
       " 'jonathan',\n",
       " 'jones',\n",
       " 'jony',\n",
       " 'jul',\n",
       " 'july',\n",
       " 'jun',\n",
       " 'june',\n",
       " 'just',\n",
       " 'kansas',\n",
       " 'kanye',\n",
       " 'kaspersky',\n",
       " 'kelly',\n",
       " 'kevin',\n",
       " 'keynote',\n",
       " 'kickstarter',\n",
       " 'kim',\n",
       " 'kindle',\n",
       " 'kodak',\n",
       " 'kong',\n",
       " 'korea',\n",
       " 'korean',\n",
       " 'kurzweil',\n",
       " 'laden',\n",
       " 'larry',\n",
       " 'last',\n",
       " 'latest',\n",
       " 'lee',\n",
       " 'leonard',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'lexus',\n",
       " 'lg',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'linkedin',\n",
       " 'linus',\n",
       " 'll',\n",
       " 'lockheed',\n",
       " 'london',\n",
       " 'looking',\n",
       " 'louis',\n",
       " 'luke',\n",
       " 'm',\n",
       " 'ma',\n",
       " \"ma'am\",\n",
       " 'mac',\n",
       " 'macbook',\n",
       " 'macos',\n",
       " 'macs',\n",
       " 'make',\n",
       " 'marc',\n",
       " 'march',\n",
       " 'marissa',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'martin',\n",
       " 'mary',\n",
       " 'mashable',\n",
       " 'mastercard',\n",
       " 'may',\n",
       " \"may've\",\n",
       " \"mayn't\",\n",
       " 'me',\n",
       " 'meerkat',\n",
       " 'meet',\n",
       " 'meg',\n",
       " 'mercedes',\n",
       " 'michael',\n",
       " 'microsoft',\n",
       " 'microsofts',\n",
       " \"might've\",\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mike',\n",
       " 'million',\n",
       " 'million-dollar',\n",
       " 'millions',\n",
       " 'mitt',\n",
       " 'monday',\n",
       " 'month',\n",
       " 'monthly',\n",
       " 'more',\n",
       " 'morning',\n",
       " 'most',\n",
       " 'motorola',\n",
       " 'mozilla',\n",
       " 'ms-dos',\n",
       " 'msft',\n",
       " 'much',\n",
       " 'musk',\n",
       " 'musks',\n",
       " 'must',\n",
       " \"must've\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " \"mustn't've\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nasa',\n",
       " 'nasas',\n",
       " 'nate',\n",
       " \"ne'er\",\n",
       " 'need',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'neil',\n",
       " 'nes',\n",
       " 'netflix',\n",
       " 'netflixes',\n",
       " 'netflixs',\n",
       " 'new',\n",
       " 'news',\n",
       " 'newsletter',\n",
       " 'nexus',\n",
       " 'nike',\n",
       " 'nikon',\n",
       " 'nintendo',\n",
       " 'nissan',\n",
       " 'no',\n",
       " 'nobel',\n",
       " 'nokia',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nov',\n",
       " 'november',\n",
       " 'now',\n",
       " 'nvidia',\n",
       " 'ny',\n",
       " 'nyc',\n",
       " 'o',\n",
       " \"o'clock\",\n",
       " \"o'er\",\n",
       " 'obama',\n",
       " 'oct',\n",
       " 'october',\n",
       " 'of',\n",
       " 'off',\n",
       " \"ol'\",\n",
       " 'oliver',\n",
       " 'olympic',\n",
       " 'olympics',\n",
       " 'on',\n",
       " 'once',\n",
       " 'oneplus',\n",
       " 'only',\n",
       " 'or',\n",
       " 'oracle',\n",
       " 'osama',\n",
       " 'oscar',\n",
       " 'oscars',\n",
       " 'osx',\n",
       " 'other',\n",
       " \"oughtn't\",\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'ozzie',\n",
       " 'p20',\n",
       " 'palmer',\n",
       " 'paris',\n",
       " 'partner',\n",
       " 'paul',\n",
       " 'pdf',\n",
       " 'peter',\n",
       " 'photoshop',\n",
       " 'pichai',\n",
       " 'pinterest',\n",
       " 'playing',\n",
       " 'playstation',\n",
       " 'plus',\n",
       " 'pokemon',\n",
       " 'polaroid',\n",
       " 'post',\n",
       " 'pr',\n",
       " 'pro',\n",
       " 'profit',\n",
       " 'ps4',\n",
       " 'puerto',\n",
       " 'punk',\n",
       " 'qualcomm',\n",
       " 'quarter',\n",
       " 'quarterly',\n",
       " 'question',\n",
       " 'questions',\n",
       " 'quora',\n",
       " 'raise',\n",
       " 're',\n",
       " 'rebecca',\n",
       " 'report',\n",
       " 'review',\n",
       " 'richard',\n",
       " 'richest',\n",
       " 'rio',\n",
       " 'robin',\n",
       " 'ron',\n",
       " 'roy',\n",
       " 'rt',\n",
       " 'rupert',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 's',\n",
       " 's2',\n",
       " 's3',\n",
       " 's4',\n",
       " 's5',\n",
       " 's6',\n",
       " 's7',\n",
       " 's8',\n",
       " 's9',\n",
       " 'sale',\n",
       " 'sales',\n",
       " 'same',\n",
       " 'samsung',\n",
       " 'san',\n",
       " 'sandberg',\n",
       " 'sarah',\n",
       " 'satya',\n",
       " 'saudi',\n",
       " 'says',\n",
       " 'scandal',\n",
       " 'scott',\n",
       " 'seattle',\n",
       " 'sep',\n",
       " 'sept',\n",
       " 'september',\n",
       " 'sergey',\n",
       " 'sf',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shares',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'siemens',\n",
       " 'siri',\n",
       " 'skype',\n",
       " 'slack',\n",
       " 'snapchat',\n",
       " 'snes',\n",
       " 'snowden',\n",
       " 'so',\n",
       " 'softbank',\n",
       " 'some',\n",
       " 'sony',\n",
       " 'spacex',\n",
       " 'spain',\n",
       " 'spanish',\n",
       " 'spielberg',\n",
       " 'sponsored',\n",
       " 'spotify',\n",
       " 'stanley',\n",
       " 'stephen',\n",
       " 'steve',\n",
       " 'stewart',\n",
       " 'story',\n",
       " 'subscribe',\n",
       " 'such',\n",
       " 'surface',\n",
       " 'sxsw',\n",
       " 't',\n",
       " 't-mobile',\n",
       " 'take',\n",
       " 'tarantino',\n",
       " 'taylor',\n",
       " 'techcrunch',\n",
       " 'telegram',\n",
       " 'tell',\n",
       " 'tells',\n",
       " 'tesla',\n",
       " 'thai',\n",
       " 'thailand',\n",
       " 'than',\n",
       " 'thanksgiven',\n",
       " 'thanksgiving',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'things',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thrones',\n",
       " 'through',\n",
       " 'thursday',\n",
       " 'tim',\n",
       " 'to',\n",
       " 'today',\n",
       " 'todays',\n",
       " 'tom',\n",
       " 'tomorrow',\n",
       " 'tony',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toshiba',\n",
       " 'toyota',\n",
       " 'trailer',\n",
       " 'travis',\n",
       " 'trending',\n",
       " 'trillion',\n",
       " 'trump',\n",
       " 'trumps',\n",
       " 'tuesday',\n",
       " 'tumblr',\n",
       " 'turkey',\n",
       " 'tweet',\n",
       " 'twitter',\n",
       " 'tzu',\n",
       " 'u2',\n",
       " 'uber',\n",
       " 'ubuntu',\n",
       " 'uk',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'us',\n",
       " 'used',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'valley',\n",
       " 'valuable',\n",
       " 'valuation',\n",
       " 've',\n",
       " 'verizon',\n",
       " 'very',\n",
       " 'via',\n",
       " 'victor',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'warner',\n",
       " 'warren',\n",
       " 'was',\n",
       " 'washington',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'waspo',\n",
       " 'we',\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'wednesday',\n",
       " 'week',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'westworld',\n",
       " 'what',\n",
       " 'whatsapp',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'wii',\n",
       " 'wikileaks',\n",
       " 'wikipedia',\n",
       " 'will',\n",
       " 'william',\n",
       " 'win',\n",
       " 'windows',\n",
       " 'wins',\n",
       " 'wired.co.uk',\n",
       " 'wired.com',\n",
       " 'with',\n",
       " 'within',\n",
       " 'wolfram',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'woods',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'writes',\n",
       " 'wwdc',\n",
       " 'xbox',\n",
       " 'xiaomi',\n",
       " 'xl',\n",
       " 'xperia',\n",
       " 'xr',\n",
       " 'xs',\n",
       " 'y',\n",
       " 'yahoo',\n",
       " 'year',\n",
       " 'year-old',\n",
       " 'years',\n",
       " 'yelp',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'york',\n",
       " 'yorke',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'youtube',\n",
       " 'zappos',\n",
       " 'zuckerberg',\n",
       " 'zuckerbergs',\n",
       " 'zynga',\n",
       " '}:'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " \"'cause\",\n",
       " 'ain',\n",
       " \"ain't\",\n",
       " 'am',\n",
       " \"amn't\",\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " \"cain't\",\n",
       " \"can't\",\n",
       " 'cant',\n",
       " \"could've\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " \"couldn't've\",\n",
       " \"daren't\",\n",
       " \"daresn't\",\n",
       " \"dasn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " \"don't\",\n",
       " 'dont',\n",
       " \"e'er\",\n",
       " \"giv'n\",\n",
       " \"gon't\",\n",
       " 'gonna',\n",
       " \"Ha'ta\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " \"he've\",\n",
       " \"how'd\",\n",
       " \"how'll\",\n",
       " \"how're\",\n",
       " \"how's\",\n",
       " 'howdy',\n",
       " \"I'd\",\n",
       " \"I'll\",\n",
       " \"I'm\",\n",
       " \"I'm'a\",\n",
       " \"I'm'o\",\n",
       " \"I've\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'isnt',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " \"let's\",\n",
       " 'ma',\n",
       " \"ma'am\",\n",
       " \"may've\",\n",
       " \"mayn't\",\n",
       " \"might've\",\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " \"must've\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " \"mustn't've\",\n",
       " \"ne'er\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " \"o'clock\",\n",
       " \"o'er\",\n",
       " \"ol'\",\n",
       " \"oughtn't\",\n",
       " 'our',\n",
       " \"S'e\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " \"she's\",\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"that'll\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " '4s',\n",
       " '5s',\n",
       " '6s',\n",
       " 'aapl',\n",
       " 'aaron',\n",
       " 'adele',\n",
       " 'adobe',\n",
       " 'ahmed',\n",
       " 'airbnb',\n",
       " 'alan',\n",
       " 'alec',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alibaba',\n",
       " 'alicia',\n",
       " 'amazon',\n",
       " 'amazons',\n",
       " 'america',\n",
       " 'american',\n",
       " 'andreesen',\n",
       " 'andrew',\n",
       " 'android',\n",
       " 'andy',\n",
       " 'angeles',\n",
       " 'aol',\n",
       " 'appl',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'ashley',\n",
       " 'ashton',\n",
       " 'assange',\n",
       " 'asus',\n",
       " 'atari',\n",
       " 'audi',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'avengers',\n",
       " 'aws',\n",
       " 'azure',\n",
       " 'ballmer',\n",
       " 'batman',\n",
       " 'bbc',\n",
       " 'beatles',\n",
       " 'berlin',\n",
       " 'bernie',\n",
       " 'bezos',\n",
       " 'bill',\n",
       " 'blackberry',\n",
       " 'bloomberg',\n",
       " 'bloomberg.com',\n",
       " 'bmw',\n",
       " 'bob',\n",
       " 'boeing',\n",
       " 'boston',\n",
       " 'bowl',\n",
       " 'brazil',\n",
       " 'british',\n",
       " 'california',\n",
       " 'canada',\n",
       " 'canon',\n",
       " 'carol',\n",
       " 'ces',\n",
       " 'charlie',\n",
       " 'chelsea',\n",
       " 'chicago',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chris',\n",
       " 'chrome',\n",
       " 'chromebook',\n",
       " 'chromecast',\n",
       " 'clinton',\n",
       " 'craig',\n",
       " 'csco',\n",
       " 'darth',\n",
       " 'david',\n",
       " 'dell',\n",
       " 'dennis',\n",
       " 'devin',\n",
       " 'dick',\n",
       " 'disney',\n",
       " 'don',\n",
       " 'donald',\n",
       " 'dorsey',\n",
       " 'dre',\n",
       " 'dropbox',\n",
       " 'ed',\n",
       " 'eduardo',\n",
       " 'edward',\n",
       " 'egypt',\n",
       " 'elizabeth',\n",
       " 'ellen',\n",
       " 'elon',\n",
       " 'england',\n",
       " 'equifax',\n",
       " 'eric',\n",
       " 'ericson',\n",
       " 'ericsson',\n",
       " 'eu',\n",
       " 'european',\n",
       " 'ev',\n",
       " 'evan',\n",
       " 'facebook',\n",
       " 'facebooks',\n",
       " 'fb',\n",
       " 'felix',\n",
       " 'ferrari',\n",
       " 'finna',\n",
       " 'firefox',\n",
       " 'forbes',\n",
       " 'forbes.com',\n",
       " 'ford',\n",
       " 'fortnite',\n",
       " 'foursquare',\n",
       " 'foxconn',\n",
       " 'france',\n",
       " 'francisco',\n",
       " 'friday',\n",
       " 'gaga',\n",
       " 'galaxy',\n",
       " 'gates',\n",
       " 'george',\n",
       " 'germany',\n",
       " 'gillmor',\n",
       " 'ginni',\n",
       " 'gizmodo',\n",
       " 'gmail',\n",
       " 'godaddy',\n",
       " 'goetz',\n",
       " 'goog',\n",
       " 'google',\n",
       " 'googles',\n",
       " 'gopro',\n",
       " 'greek',\n",
       " 'groupon',\n",
       " 'haiti',\n",
       " 'harry',\n",
       " 'hbo',\n",
       " 'hbos',\n",
       " 'hilary',\n",
       " 'houston',\n",
       " 'hp',\n",
       " 'htc',\n",
       " 'huawei',\n",
       " 'huffington',\n",
       " 'huffpo',\n",
       " 'hugo',\n",
       " 'hulu',\n",
       " 'ibm',\n",
       " 'ibooks',\n",
       " 'icloud',\n",
       " 'ikea',\n",
       " 'imac',\n",
       " 'instagram',\n",
       " 'intel',\n",
       " 'ios',\n",
       " 'ipad',\n",
       " 'ipads',\n",
       " 'iphone',\n",
       " 'ipod',\n",
       " 'ireland',\n",
       " 'irish',\n",
       " 'italia',\n",
       " 'italy',\n",
       " 'itunes',\n",
       " 'iwatch',\n",
       " 'jack',\n",
       " 'james',\n",
       " 'japan',\n",
       " 'jedi',\n",
       " 'jeff',\n",
       " 'jerry',\n",
       " 'jk',\n",
       " 'jobs',\n",
       " 'john',\n",
       " 'johnatan',\n",
       " 'jon',\n",
       " 'jonathan',\n",
       " 'jones',\n",
       " 'jony',\n",
       " 'kansas',\n",
       " 'kanye',\n",
       " 'kaspersky',\n",
       " 'kelly',\n",
       " 'kevin',\n",
       " 'kickstarter',\n",
       " 'kim',\n",
       " 'kindle',\n",
       " 'kodak',\n",
       " 'kong',\n",
       " 'korea',\n",
       " 'korean',\n",
       " 'kurzweil',\n",
       " 'laden',\n",
       " 'larry',\n",
       " 'lee',\n",
       " 'leonard',\n",
       " 'lexus',\n",
       " 'lg',\n",
       " 'linkedin',\n",
       " 'linus',\n",
       " 'lockheed',\n",
       " 'london',\n",
       " 'louis',\n",
       " 'luke',\n",
       " 'mac',\n",
       " 'macbook',\n",
       " 'macos',\n",
       " 'macs',\n",
       " 'marc',\n",
       " 'marissa',\n",
       " 'mark',\n",
       " 'martin',\n",
       " 'mary',\n",
       " 'mashable',\n",
       " 'mastercard',\n",
       " 'meerkat',\n",
       " 'meg',\n",
       " 'mercedes',\n",
       " 'michael',\n",
       " 'microsoft',\n",
       " 'microsofts',\n",
       " 'mike',\n",
       " 'mitt',\n",
       " 'motorola',\n",
       " 'mozilla',\n",
       " 'ms-dos',\n",
       " 'msft',\n",
       " 'musk',\n",
       " 'musks',\n",
       " 'nasa',\n",
       " 'nasas',\n",
       " 'nate',\n",
       " 'neil',\n",
       " 'nes',\n",
       " 'netflix',\n",
       " 'netflixes',\n",
       " 'netflixs',\n",
       " 'nexus',\n",
       " 'nike',\n",
       " 'nikon',\n",
       " 'nintendo',\n",
       " 'nissan',\n",
       " 'nokia',\n",
       " 'nvidia',\n",
       " 'ny',\n",
       " 'nyc',\n",
       " 'obama',\n",
       " 'oliver',\n",
       " 'oneplus',\n",
       " 'oracle',\n",
       " 'osama',\n",
       " 'osx',\n",
       " 'ozzie',\n",
       " 'p20',\n",
       " 'palmer',\n",
       " 'paris',\n",
       " 'paul',\n",
       " 'pdf',\n",
       " 'peter',\n",
       " 'photoshop',\n",
       " 'pichai',\n",
       " 'pinterest',\n",
       " 'playstation',\n",
       " 'pokemon',\n",
       " 'polaroid',\n",
       " 'pro',\n",
       " 'ps4',\n",
       " 'puerto',\n",
       " 'punk',\n",
       " 'qualcomm',\n",
       " 'quora',\n",
       " 'rebecca',\n",
       " 'richard',\n",
       " 'rio',\n",
       " 'robin',\n",
       " 'ron',\n",
       " 'roy',\n",
       " 'rupert',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 's2',\n",
       " 's3',\n",
       " 's4',\n",
       " 's5',\n",
       " 's6',\n",
       " 's7',\n",
       " 's8',\n",
       " 's9',\n",
       " 'samsung',\n",
       " 'san',\n",
       " 'sandberg',\n",
       " 'sarah',\n",
       " 'satya',\n",
       " 'saudi',\n",
       " 'scott',\n",
       " 'seattle',\n",
       " 'sergey',\n",
       " 'sf',\n",
       " 'siemens',\n",
       " 'siri',\n",
       " 'skype',\n",
       " 'slack',\n",
       " 'snapchat',\n",
       " 'snes',\n",
       " 'snowden',\n",
       " 'softbank',\n",
       " 'sony',\n",
       " 'spacex',\n",
       " 'spain',\n",
       " 'spanish',\n",
       " 'spielberg',\n",
       " 'spotify',\n",
       " 'stanley',\n",
       " 'stephen',\n",
       " 'steve',\n",
       " 'stewart',\n",
       " 'surface',\n",
       " 'sxsw',\n",
       " 't-mobile',\n",
       " 'tarantino',\n",
       " 'taylor',\n",
       " 'techcrunch',\n",
       " 'telegram',\n",
       " 'tesla',\n",
       " 'thai',\n",
       " 'thailand',\n",
       " 'thrones',\n",
       " 'tim',\n",
       " 'tom',\n",
       " 'tony',\n",
       " 'toshiba',\n",
       " 'toyota',\n",
       " 'travis',\n",
       " 'trump',\n",
       " 'trumps',\n",
       " 'tumblr',\n",
       " 'turkey',\n",
       " 'tweet',\n",
       " 'twitter',\n",
       " 'tzu',\n",
       " 'u2',\n",
       " 'uber',\n",
       " 'ubuntu',\n",
       " 'uk',\n",
       " 'us',\n",
       " 'valley',\n",
       " 'verizon',\n",
       " 'victor',\n",
       " 'warner',\n",
       " 'warren',\n",
       " 'washington',\n",
       " 'waspo',\n",
       " 'westworld',\n",
       " 'whatsapp',\n",
       " 'wii',\n",
       " 'wikileaks',\n",
       " 'wikipedia',\n",
       " 'william',\n",
       " 'windows',\n",
       " 'wired.co.uk',\n",
       " 'wired.com',\n",
       " 'wolfram',\n",
       " 'woods',\n",
       " 'xbox',\n",
       " 'xiaomi',\n",
       " 'xl',\n",
       " 'xperia',\n",
       " 'xr',\n",
       " 'xs',\n",
       " 'yahoo',\n",
       " 'yelp',\n",
       " 'york',\n",
       " 'yorke',\n",
       " 'youtube',\n",
       " 'zappos',\n",
       " 'zuckerberg',\n",
       " 'zuckerbergs',\n",
       " 'zynga',\n",
       " '-->',\n",
       " ':p',\n",
       " '...',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '):',\n",
       " ']:',\n",
       " '}:',\n",
       " '/:',\n",
       " '<u+>',\n",
       " 'billion',\n",
       " 'billionaire',\n",
       " 'billions',\n",
       " 'earnings',\n",
       " 'funding',\n",
       " 'investment',\n",
       " 'investor',\n",
       " 'million',\n",
       " 'million-dollar',\n",
       " 'millions',\n",
       " 'profit',\n",
       " 'raise',\n",
       " 'trillion',\n",
       " 'valuation',\n",
       " 'valuable',\n",
       " 'worth',\n",
       " 'ago',\n",
       " 'anniversary',\n",
       " 'annual',\n",
       " 'anytime',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'around',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'awards',\n",
       " 'birthday',\n",
       " 'christmas',\n",
       " 'cup',\n",
       " 'december',\n",
       " 'earthquake',\n",
       " 'election',\n",
       " 'elections',\n",
       " 'episode',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'feb',\n",
       " 'february',\n",
       " 'hurricane',\n",
       " 'ipo',\n",
       " 'jan',\n",
       " 'january',\n",
       " 'jul',\n",
       " 'july',\n",
       " 'jun',\n",
       " 'june',\n",
       " 'march',\n",
       " 'may',\n",
       " 'monday',\n",
       " 'month',\n",
       " 'monthly',\n",
       " 'morning',\n",
       " 'nobel',\n",
       " 'nov',\n",
       " 'november',\n",
       " 'oct',\n",
       " 'october',\n",
       " 'olympic',\n",
       " 'olympics',\n",
       " 'once',\n",
       " 'oscar',\n",
       " 'oscars',\n",
       " 'quarter',\n",
       " 'quarterly',\n",
       " 'sep',\n",
       " 'sept',\n",
       " 'september',\n",
       " 'thanksgiven',\n",
       " 'thanksgiving',\n",
       " 'thursday',\n",
       " 'today',\n",
       " 'todays',\n",
       " 'tomorrow',\n",
       " 'tuesday',\n",
       " 'wednesday',\n",
       " 'week',\n",
       " 'when',\n",
       " 'wwdc',\n",
       " 'year',\n",
       " 'year-old',\n",
       " 'years',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'acquisition',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'already',\n",
       " 'also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'another',\n",
       " 'answers',\n",
       " 'any',\n",
       " 'anyone',\n",
       " 'are',\n",
       " 'arrive',\n",
       " 'arrives',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'asks',\n",
       " 'at',\n",
       " 'available',\n",
       " 'be',\n",
       " 'because',\n",
       " 'become',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'best',\n",
       " 'between',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'both',\n",
       " 'briefing',\n",
       " 'bring',\n",
       " 'brings',\n",
       " 'build',\n",
       " 'building',\n",
       " 'builds',\n",
       " 'built',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'buys',\n",
       " 'by',\n",
       " 'can',\n",
       " 'ceo',\n",
       " 'co-founder',\n",
       " 'content',\n",
       " 'could',\n",
       " 'daily',\n",
       " 'day',\n",
       " 'deals',\n",
       " 'did',\n",
       " 'dislike',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downloading',\n",
       " 'during',\n",
       " 'each',\n",
       " 'else',\n",
       " 'employee',\n",
       " 'employees',\n",
       " 'every',\n",
       " \"everyone's\",\n",
       " 'example',\n",
       " 'examples',\n",
       " 'exponentially',\n",
       " 'few',\n",
       " 'for',\n",
       " 'founder',\n",
       " 'free',\n",
       " 'from',\n",
       " 'fund',\n",
       " 'further',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'gift',\n",
       " 'gimme',\n",
       " 'goes',\n",
       " 'gotta',\n",
       " 'guide',\n",
       " 'had',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hottest',\n",
       " 'how',\n",
       " 'if',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'important',\n",
       " 'in',\n",
       " 'insider',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keynote',\n",
       " 'last',\n",
       " 'latest',\n",
       " 'let',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'll',\n",
       " 'looking',\n",
       " 'make',\n",
       " 'market',\n",
       " 'me',\n",
       " 'meet',\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'need',\n",
       " 'new',\n",
       " 'news',\n",
       " 'newsletter',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'partner',\n",
       " 'playing',\n",
       " 'plus',\n",
       " 'post',\n",
       " 'pr',\n",
       " 'question',\n",
       " 'questions',\n",
       " 're',\n",
       " 'report',\n",
       " 'review',\n",
       " 'richest',\n",
       " 'rt',\n",
       " 'sale',\n",
       " 'sales',\n",
       " 'same',\n",
       " 'says',\n",
       " 'scandal',\n",
       " 'shares',\n",
       " 'she',\n",
       " 'should',\n",
       " 'so',\n",
       " 'some',\n",
       " 'sponsored',\n",
       " 'story',\n",
       " 'subscribe',\n",
       " 'such',\n",
       " 'take',\n",
       " 'tell',\n",
       " 'tells',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'things',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'top',\n",
       " 'trailer',\n",
       " 'trending',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'used',\n",
       " 'uses',\n",
       " 'using',\n",
       " 've',\n",
       " 'very',\n",
       " 'via',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were',\n",
       " 'what',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'win',\n",
       " 'wins',\n",
       " 'with',\n",
       " 'within',\n",
       " 'worst',\n",
       " 'would',\n",
       " 'writes',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cleantokens['Unigrams'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fox time',\n",
       " 'time warner',\n",
       " 'warner spat',\n",
       " 'spat shows',\n",
       " 'shows death',\n",
       " 'death tv',\n",
       " 'tv cannot',\n",
       " 'cannot come',\n",
       " 'come soon',\n",
       " 'soon enough']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{tuple[0]} {tuple[1]}' for tuple in list(bigrams(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [tuple[0] for tuple in config.items('Stop Words')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n'cause\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"Stop Words\"][\"contractions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + contractions + named_entities + emojis + money + time_event + too_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleantokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBigrams(df):\n",
    "    tqdm.write('.... generating bigrams')\n",
    "    start = time.time()\n",
    "    df['Bigrams'] = df['Unigrams'].progress_apply(lambda x: [f'{tuple[0]} {tuple[1]}' for tuple in list(bigrams(x))])\n",
    "    df['NumTokens']=df['Unigrams'].apply(len)\n",
    "    df['NumBigrams']=df['Bigrams'].apply(len)\n",
    "#     df=df[df['NumTokens']<40]\n",
    "#     df=df[df['NumTokens']>1]\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Tokens cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = 'tokenized.data'\n",
    "generateBigrams(df).to_pickle(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
