{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from contractionExpander import expand_text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    start = time.time()\n",
    "    tqdm.pandas()\n",
    "    print ('Running setup...')\n",
    "    resources = ['taggers/averaged_perceptron_tagger', 'corpora/wordnet', 'corpora/stopwords', 'tokenizers/punkt']\n",
    "    for path in resources:\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            nltk.download(path.split('/')[1])\n",
    "    end = time.time()\n",
    "    print (f'Setup finished in {end-start:.2f} seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup...\n",
      "Setup finished in 0.00 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_to_date(srl_no, first=datetime.datetime(2010,1,1,0,0)):\n",
    "    days = int(srl_no-1)\n",
    "    new_date = first + datetime.timedelta(days)\n",
    "    return new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    print('Reading data...')\n",
    "    start = time.time()\n",
    "    tweets = pd.read_csv(filename, encoding='latin-1', sep=';',header=0, names=['StringDate', 'Days', 'From', 'Tweet'])\n",
    "    tweets = tweets.filter(items=['Days', 'From', 'Tweet'])\n",
    "    tweets['Days'] = tweets['Days'].progress_apply(days_to_date)\n",
    "    tweets.columns=['Date', 'From', 'Tweet']\n",
    "    tweets = tweets.reset_index()\n",
    "    tweets = tweets.set_index('Date').sort_index()\n",
    "    end = time.time()\n",
    "    print (f'Data read in {end-start:.2f} seconds.\\n')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:01<00:00, 407592.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data read in 3.07 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = 'twitter.csv'\n",
    "data = read_data(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>From</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>65282</td>\n",
       "      <td>@SAI</td>\n",
       "      <td>FOX -Time Warner Spat Shows Why The Death Of T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502287</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>Ten Technologies That Will Rock 2010 http://tc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>502288</td>\n",
       "      <td>@TechCrunch</td>\n",
       "      <td>Hotel WiFi Should Be a Right, Not a Luxury htt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index         From  \\\n",
       "Date                              \n",
       "2010-01-01   65282         @SAI   \n",
       "2010-01-01  502287  @TechCrunch   \n",
       "2010-01-01  502288  @TechCrunch   \n",
       "\n",
       "                                                        Tweet  \n",
       "Date                                                           \n",
       "2010-01-01  FOX -Time Warner Spat Shows Why The Death Of T...  \n",
       "2010-01-01  Ten Technologies That Will Rock 2010 http://tc...  \n",
       "2010-01-01  Hotel WiFi Should Be a Right, Not a Luxury htt...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    text = unicodedata.normalize('NFD', str(text)).encode('ascii', 'ignore').decode(\"utf-8\").lower()\n",
    "    return str(text)\n",
    "\n",
    "def remove_apostrophes(text):\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    #hashtags and handles\n",
    "    text = re.sub(r'\\B(\\#[a-zA-Z]+|\\@[a-zA-Z]+\\b)', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    text= re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_numberwords(text):\n",
    "    text= re.sub(r'\\b[0-9]+\\b\\s*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, text_column):\n",
    "    tqdm.write(f'Cleaning up {text_column} texts...')\n",
    "    start = time.time()\n",
    "    tqdm.write('.... removing accents')\n",
    "    df[text_column] = df[text_column].progress_apply(remove_accents)\n",
    "    tqdm.write('.... removing URLs')\n",
    "    df[text_column] = df[text_column].progress_apply(remove_urls)\n",
    "    tqdm.write('.... removing hashtags')\n",
    "    df[text_column] = df[text_column].progress_apply(remove_hashtags)\n",
    "    tqdm.write('.... removing apostrophes')\n",
    "    df[text_column] = df[text_column].progress_apply(remove_apostrophes) \n",
    "    tqdm.write('.... removing numbers')\n",
    "    df[text_column] = df[text_column].progress_apply(remove_numberwords) \n",
    "    tqdm.write('.... expanding contractions')\n",
    "    df[text_column] = df[text_column].progress_apply(expand_text)\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Text cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15848/645981 [00:00<00:03, 158472.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Tweet texts...\n",
      ".... removing accents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:02<00:00, 317651.93it/s]\n",
      "  8%|▊         | 51402/645981 [00:00<00:02, 242424.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:02<00:00, 289635.77it/s]\n",
      "  5%|▌         | 34160/645981 [00:00<00:03, 163328.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing hashtags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:03<00:00, 174042.07it/s]\n",
      "  4%|▍         | 27153/645981 [00:00<00:02, 271529.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing apostrophes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:01<00:00, 340264.19it/s]\n",
      "  5%|▌         | 33893/645981 [00:00<00:03, 160984.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:03<00:00, 179268.27it/s]\n",
      "  0%|          | 970/645981 [00:00<02:20, 4599.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... expanding contractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [02:16<00:00, 4727.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleanup finished in 150.45 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned = clean_text(data, 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned.to_pickle('cleaned.data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize(text):\n",
    "    new_text = []\n",
    "    for word, tag in pos_tag(tknzr.tokenize(text)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 's'] else None\n",
    "        if wntag:  # remove verbs\n",
    "            lemma = lmtzr.lemmatize(word, wntag)\n",
    "            new_text.append(lemma)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def tokenize(df, text_column):\n",
    "    print(f'Tokenizing Dataframe[\"{text_column}\"].')\n",
    "    start = time.time()\n",
    "    #df['Unigrams'] = df[text_column].progress_apply(_tokenize)\n",
    "    df['Unigrams'] = df[text_column].progress_apply(tknzr.tokenize)\n",
    "    end = time.time()\n",
    "    print(f'Dataframe[\"{text_column}\"] tokenized in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 317/645981 [00:00<03:32, 3038.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Dataframe[\"Tweet\"].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:53<00:00, 12147.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe[\"Tweet\"] tokenized in 53.44 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens = tokenize(cleaned, 'Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation) + [' ', 'rt', '...', '-->', ']:', '}:'])\n",
    "    output = [i for i in input if i not in stop_words]\n",
    "    return output\n",
    "\n",
    "def remove_extremewords(input):\n",
    "    output = [i for i in input if (len(i)<20 and len(i)>1)]   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(df):\n",
    "    tqdm.write('Cleaning up tokens...')\n",
    "    start = time.time()\n",
    "    tqdm.write('.... removing stop words')\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(remove_stopwords)\n",
    "    tqdm.write('.... removing extreme words')\n",
    "    df['Unigrams'] = df['Unigrams'].progress_apply(remove_extremewords)\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Tokens cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 649/645981 [00:00<04:44, 2265.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up tokens...\n",
      ".... removing stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [02:06<00:00, 5119.29it/s]\n",
      "  3%|▎         | 19606/645981 [00:00<00:03, 195979.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... removing extreme words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 645981/645981 [00:03<00:00, 210588.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens cleanup finished in 129.56 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleantokens = clean_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cleantokens['Unigrams'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fox time',\n",
       " 'time warner',\n",
       " 'warner spat',\n",
       " 'spat shows',\n",
       " 'shows death',\n",
       " 'death tv',\n",
       " 'tv cannot',\n",
       " 'cannot come',\n",
       " 'come soon',\n",
       " 'soon enough']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{tuple[0]} {tuple[1]}' for tuple in list(bigrams(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleantokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBigrams(df):\n",
    "    tqdm.write('.... generating bigrams')\n",
    "    start = time.time()\n",
    "    df['Bigrams'] = df['Unigrams'].progress_apply(lambda x: [f'{tuple[0]} {tuple[1]}' for tuple in list(bigrams(x))])\n",
    "    df['NumTokens']=df['Unigrams'].apply(len)\n",
    "    df['NumBigrams']=df['Bigrams'].apply(len)\n",
    "#     df=df[df['NumTokens']<40]\n",
    "#     df=df[df['NumTokens']>1]\n",
    "    end = time.time()\n",
    "    tqdm.write (f'Tokens cleanup finished in {end-start:.2f} seconds.\\n')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = 'tokenized.data'\n",
    "generateBigrams(df).to_pickle(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
